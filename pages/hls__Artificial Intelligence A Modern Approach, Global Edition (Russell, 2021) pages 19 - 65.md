file:: [Artificial Intelligence A Modern Approach, Global Edition (Russell, 2021) pages 19 - 65.pdf](../assets/Artificial Intelligence A Modern Approach, Global Edition (Russell, 2021) pages 19 - 65.pdf)
file-path:: ../assets/Artificial Intelligence A Modern Approach, Global Edition (Russell, 2021) pages 19 - 65.pdf

- Chapter 1 - Introduction
  hl-page:: 1
  ls-type:: annotation
  id:: 67c2612d-5964-4e19-8def-fceb67b6ad0c
  hl-color:: red
	- We call ourselves Homo sapiens—man the wise—because our **intelligence** is so important to us.
	  hl-page:: 1
	  ls-type:: annotation
	  id:: 67c2615c-e9bb-4e59-ad96-6d356b565fd5
	  hl-color:: blue
	  collapsed:: true
		- For thousands of years, we have tried to understand *how we think and act*—that is, how our brain, a mere handful of matter, can perceive, understand, predict, and manipulate a world far larger and more complicated than itself
		  ls-type:: annotation
		  hl-page:: 1
		  hl-color:: yellow
		  id:: 67c26179-f254-46f3-85bf-bfb9b461d929
		- The ﬁeld of **artiﬁcial intelligence**, or AI, is concerned with not just understanding but also building intelligent entities—machines that can compute how to act effectively and safely in a wide variety of novel situations.
		  hl-page:: 1
		  ls-type:: annotation
		  id:: 67c2619e-ea5a-47bd-821f-dd6595aaef1b
		  hl-color:: yellow
	- Surveys regularly rank AI as one of the most interesting and fastest-growing ﬁelds, and it is already generating over a trillion dollars a year in revenue.
	  ls-type:: annotation
	  hl-page:: 1
	  hl-color:: blue
	  id:: 67c261d1-d775-4f1f-a362-9890a1232594
	  collapsed:: true
		- AI expert Kai-Fu Lee predicts that its impact will be “more than anything in the history of mankind.” Moreover, the intellectual frontiers of AI are wide open.
		  ls-type:: annotation
		  hl-page:: 1
		  hl-color:: yellow
		  id:: 67c26256-3bf3-40d0-8071-78558b1f73a9
		- Whereas a student of an older science such as physics might feel that the best ideas have already been discovered by Galileo, Newton, Curie, Einstein, and the rest, AI still has many openings for full-time masterminds.
		  ls-type:: annotation
		  hl-page:: 1
		  hl-color:: yellow
		  id:: 67c26269-3a27-4d28-b8c8-7da1ee0309f3
	- AI currently encompasses a huge variety of subﬁelds, ranging from the general (learning, reasoning, perception, and so on) to the speciﬁc, such as playing chess, proving mathematical theorems, writing poetry, driving a car, or diagnosing diseases. AI is relevant to any intellectual task; it is truly a universal ﬁeld.
	  ls-type:: annotation
	  hl-page:: 1
	  hl-color:: blue
	  id:: 67c26286-8477-4269-aeb4-4dbcca9dd441
	- 1.1 What Is AI?
	  ls-type:: annotation
	  hl-page:: 1
	  hl-color:: red
	  id:: 67c26296-96fc-4439-9734-993ae22073da
	  collapsed:: true
		- We have claimed that AI is interesting, but we have not said what it is.
		  ls-type:: annotation
		  hl-page:: 1
		  hl-color:: blue
		  id:: 67c262a7-990c-46a0-a4bb-25033dcbeb3c
		  collapsed:: true
			- Historically, researchers have pursued several different versions of AI. Some have deﬁned intelligence in terms of ﬁdelity to human performance, while others prefer an abstract, formal deﬁnition of intelligence called:
			  ls-type:: annotation
			  hl-page:: 1
			  hl-color:: yellow
			  id:: 67c262bf-f095-42b5-9c8a-79d49acb81bd
			- **rationality**—loosely speaking, doing the “right thing.” The subject matter Rationality itself also varies: some consider intelligence to be a property of internal thought processes and reasoning, while others focus on intelligent behavior, an external characterization.[^1]
			  hl-page:: 1
			  ls-type:: annotation
			  id:: 67c262d3-01bf-4b76-9d06-6597cdec236b
			  hl-color:: green
				- hl-page:: 1
				  ls-type:: annotation
				  id:: 67c26312-4c02-41c1-83f0-ff8566537c1f
				  hl-color:: purple
				  hl-stamp:: 1740792672097
				  [^1]:In the public eye, there is sometimes confusion between the terms “artiﬁcial intelligence” and “machine learning.” Machine learning is a subﬁeld of AI that studies the ability to improve performance based on experience. Some AI systems use machine learning methods to achieve competence, but some do not.
		- From these two dimensions—human vs. rational[^2] and thought vs. behavior—there are four possible combinations, and there have been adherents and research programs for all four.
		  hl-page:: 1
		  ls-type:: annotation
		  id:: 67c26326-4acd-4bbf-9994-53df7478f6e3
		  hl-color:: blue
		  collapsed:: true
			- The methods used are necessarily different: the pursuit of human-like intelligence must be in part an empirical science related to psychology, involving observations and hypotheses about actual human behavior and thought processes; a rationalist approach, on the other hand, involves a combination of mathematics and engineering, and connects to statistics, control theory, and economics.
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: yellow
			  id:: 67c263c0-7c96-4e43-96c7-46e8bb867556
			- The various groups have both disparaged and helped each other. Let us look at the four approaches in more detail.
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: yellow
			  id:: 67c263c9-0d87-4808-9316-2c8a142879b5
			- hl-page:: 1
			  ls-type:: annotation
			  id:: 67c2635d-e87e-40cb-a72c-869f03713613
			  hl-color:: purple
			  [^2]:We are not suggesting that humans are “irrational” in the dictionary sense of “deprived of normal mental clarity.” We are merely conceding that human decisions are not always mathematically perfect.
		- 1.1.1 Acting humanly: The Turing test approach
		  ls-type:: annotation
		  hl-page:: 2
		  hl-color:: red
		  id:: 67c263f3-a8f6-43d7-beba-a044b66cb059
		  collapsed:: true
			- The **Turing test**, proposed by Alan Turing (1950), was designed as a thought experiment thatTuring test would sidestep the philosophical vagueness of the question “Can a machine think?
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: green
			  id:: 67c26420-84bc-47fe-8bd4-f785cdd57fab
			  collapsed:: true
				- A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer. Chapter 28 discusses the details of the test and whether a computer would really be intelligent if it passed.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: yellow
				  id:: 67c26442-fbdf-4b83-8cef-a500958fc50e
			- For now, we note that programming a computer to pass a rigorously applied test provides plenty to work on. The computer would need the following capabilities:
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: green
			  id:: 67c26455-35ea-4175-825a-969362e70274
			  hl-stamp:: 1740792925206
			  collapsed:: true
				- **natural language processing** to communicate successfully in a human language;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26479-0d66-44d4-88fa-98c9b3b07fbd
				- **knowledge representation** to store what it knows or hears;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26487-f746-4560-a935-da0c1e69d34e
				- **automated reasoning** to answer questions and to draw new conclusions;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26496-1246-4e34-abe4-f32b4baf880f
				- **machine learning** to adapt to new circumstances and to detect and extrapolate patterns.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c264a6-4ab8-4a2d-8d75-fb9855410bb0
			- Turing viewed the *physical* simulation of a person as unnecessary to demonstrate intelligence.
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: blue
			  id:: 67c265c0-7598-484c-a09d-e95ef32a9043
			- However, other researchers have proposed a **total Turing test**, which requires interaction withTotal Turing test objects and people in the real world. To pass the total Turing test, a robot will need
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: green
			  id:: 67c26605-a21e-4528-9d10-97e7b355c99e
			  collapsed:: true
				- **computer vision** and speech recognition to perceive the world;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c2662a-4721-4dcf-946b-9245814b01cd
				- **robotics** to manipulate objects and move about.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26637-a688-40f8-b256-f0a002272c0a
			- These six disciplines compose most of AI. Yet AI researchers have devoted little effort to passing the Turing test, believing that it is more important to study the underlying principles of intelligence.
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: blue
			  id:: 67c26655-00f4-4a6b-aaa3-0b27d1c07f00
				- The quest for “artiﬁcial ﬂight” succeeded when engineers and inventors stopped imitating birds and started using wind tunnels and learning about aerodynamics.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: yellow
				  id:: 67c266a7-b3f1-4cb8-aeb4-b9a7e7c15778
				- Aeronautical engineering texts do not deﬁne the goal of their ﬁeld as making “machines thatﬂy so exactly like pigeons that they can fool even other pigeons.”
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: yellow
				  id:: 67c266b2-ae44-4312-a990-da62f2f293aa
		- 1.1.2 Thinking humanly: The cognitive modeling approach
		  ls-type:: annotation
		  hl-page:: 2
		  hl-color:: red
		  id:: 67c266d8-ced1-4bab-b2ce-e81ea5be2ab4
		  collapsed:: true
			- To say that a program thinks like a human, we must know how humans think. We can learn about human thought in three ways:
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: blue
			  id:: 67c266ff-343b-44fe-9c11-14e29d7412a2
			  collapsed:: true
				- **introspection**—trying to catch our own thoughts as they go by;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26766-9473-4d7c-adc0-4e62679a9fdb
				- **psychological experiments**—observing a person in action;
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c26787-9d44-4496-b692-5478b2edfabb
				- **brain imaging**—observing the brain in action.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: green
				  id:: 67c267b1-53d9-4746-8900-0db35edc5ab2
			- Once we have a sufﬁciently precise theory of the mind, it becomes possible to express the theory as a computer program.
			  ls-type:: annotation
			  hl-page:: 2
			  hl-color:: blue
			  id:: 67c267e6-85a1-4dbf-85be-a14d77fe1763
			  collapsed:: true
				- If the program’s input–output behavior matches corresponding human behavior, that is evidence that some of the program’s mechanisms could also be operating in humans.
				  ls-type:: annotation
				  hl-page:: 2
				  hl-color:: yellow
				  id:: 67c26866-0b52-4ad5-89f7-7cf38b4c00c0
			- For example, Allen Newell and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon, 1961), were not content merely to have their program solve problems correctly.
			  hl-page:: 2
			  ls-type:: annotation
			  id:: 67c26879-c7b1-4c94-80fa-b27e99e48639
			  hl-color:: blue
			  collapsed:: true
				- They were more concerned with comparing the sequence and timing of its reasoning steps to those of human subjects solving the same problems
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c26891-23ee-49da-b967-343fb56d15fb
			- The interdisciplinary ﬁeld of **cognitive science** brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind.
			  hl-page:: 3
			  ls-type:: annotation
			  id:: 67c2689e-d4cd-4f25-9875-6544f9853478
			  hl-color:: green
			- Cognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least one encyclopedia (Wilson and Keil, 1999).
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: blue
			  id:: 67c268ea-b5a3-4549-932e-6018bacdcbee
			  collapsed:: true
				- We will occasionally comment on similarities or differences between AI techniques and human cognition. 
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c268fb-a870-4087-b370-ca59c4a42d99
				- Real cognitive science, however, is necessarily based on experimental investigation of actual humans or animals. We will leave that for other books, as we assume the reader has only a computer for experimentation.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c26917-35aa-4a2e-85ca-cfaab0f95a36
			- In the early days of AI there was often confusion between the approaches. An author would argue that an algorithm performs well on a task and that it is *therefore* a good model of human performance, or vice versa.
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: blue
			  id:: 67c26948-19ce-4d42-b3eb-6a378eaf2e78
				- Modern authors separate the two kinds of claims; this distinction has allowed both AI and cognitive science to develop more rapidly.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c2695f-054a-4c70-98b4-a19404ddfa7e
				- The two ﬁelds fertilize each other, most notably in computer vision, which incorporates neurophysiological evidence into computational models.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c269b3-9df8-4ac6-acbf-735a2f8d831f
				- Recently, the combination of neuroimaging methods combined with machine learning techniques for analyzing such data has led to the beginnings of a capability to “read minds”—that is, to ascertain the semantic content of a person’s inner thoughts. This capability could, in turn, shed further light on how human cognition works.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c269d0-3ff3-493d-a979-bdbe28577808
		- 1.1.3 Thinking rationally: The “laws of thought” approach
		  ls-type:: annotation
		  hl-page:: 3
		  hl-color:: red
		  id:: 67c269e1-d009-4476-8221-3bda36c35de6
		  collapsed:: true
			- The Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking”— that is, irrefutable reasoning processes.
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: blue
			  id:: 67c269fb-1070-4a2b-99a8-57a3d89a5692
			  collapsed:: true
				- His **syllogisms** provided patterns for argument structures that always yielded correct conclusions when given correct premises
				  hl-page:: 3
				  ls-type:: annotation
				  id:: 67c26aab-fda6-4598-a320-645752cc7159
				  hl-color:: green
				  collapsed:: true
					- The canonical example starts with *Socrates is a man and all men are mortal* and concludes that *Socrates is mortal*. (This example is probably due to Sextus Empiricus rather than Aristotle.)
					  hl-page:: 3
					  ls-type:: annotation
					  id:: 67c26ad5-d8c3-4ef4-a951-0cac349d0f97
					  hl-color:: yellow
				- These laws of thought were supposed to govern the operation of the mind; their study initiated the ﬁeld called **logic**.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: green
				  id:: 67c26af5-3a3f-4b10-94d8-fbd0fcb9f563
			- Logicians in the 19th century developed a precise notation for statements about objects in the world and the relations among them. (Contrast this with ordinary arithmetic notation, which provides only for statements about numbers.)
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: blue
			  id:: 67c26b29-0618-49f2-ae3f-03b41b27b506
			  collapsed:: true
				- By 1965, programs could, in principle, solve any solvable problem described in logical notation.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c26b63-f99e-4e63-b36d-e3c6d549b801
			- The so-called **logicist** tradition within artiﬁcial intelligence hopes to build on such programs to create intelligent systems.
			  hl-stamp:: 1740794738709
			  hl-page:: 3
			  ls-type:: annotation
			  id:: 67c26b70-9cd4-4d35-9f50-e20937dcf2ad
			  hl-color:: green
			- Logic as conventionally understood requires knowledge of the world that is *certain*—a condition that, in reality, is seldom achieved.
			  hl-page:: 3
			  ls-type:: annotation
			  id:: 67c26b95-0de2-4224-9103-cb708e02cf02
			  hl-color:: blue
			  collapsed:: true
				- We simply don’t know the rules of, say, politics or warfare in the same way that we know the rules of chess or arithmetic.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c26bcd-e776-4650-901d-b8def33d9637
			- The theory of **probability** ﬁlls this gap, allowing rigorous reasoning with uncertain information.
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: green
			  id:: 67c26bdb-b644-44dd-abf9-44a33075135e
			  collapsed:: true
				- In principle, it allows the construction of a comprehensive model of rational thought, leading from raw perceptual information to an understanding of how the world works to predictions about the future.
				  ls-type:: annotation
				  hl-page:: 3
				  hl-color:: yellow
				  id:: 67c26d4a-7032-49ab-a4be-c5b9a3ea6839
			- What it does not do, is generate intelligent *behavior*. For that, we need a theory of rational action. Rational thought, by itself, is not enough.
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: blue
			  id:: 67c26d68-947b-412d-862a-330144043dfb
			  hl-stamp:: 1740795249341
		- 1.1.4 Acting rationally: The rational agent approach
		  ls-type:: annotation
		  hl-page:: 3
		  hl-color:: red
		  id:: 67c26d84-25e1-46f6-9e0c-3f1b013544fb
		  collapsed:: true
			- An **agent** is just something that acts (agent comes from the Latin agere, to do).
			  ls-type:: annotation
			  hl-page:: 3
			  hl-color:: green
			  id:: 67c26db2-377e-4f0c-a356-4b8210946856
			  collapsed:: true
				- Of course, all computer programs do something, but computer agents are expected to do more: operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals.
				  hl-page:: 4
				  ls-type:: annotation
				  id:: 67c26dd3-465e-4abf-88fa-c1e516ee984c
				  hl-color:: yellow
			- A **rational agent** is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome.
			  hl-page:: 4
			  ls-type:: annotation
			  id:: 67c26de3-4928-49d4-b8d0-4cef1f5da99a
			  hl-color:: green
			- In the “laws of thought” approach to AI, the emphasis was on correct inferences.
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c26e27-c78d-409f-9b84-7718655ed78e
			  collapsed:: true
				- Making correct inferences is sometimes *part* of being a rational agent, because one way to act rationally is to deduce that a given action is best and then to act on that conclusion.
				  hl-page:: 4
				  ls-type:: annotation
				  id:: 67c279f5-562e-4b0f-821d-dc50ca572dd9
				  hl-color:: yellow
				- On the other hand, there are ways of acting rationally that cannot be said to involve inference. For example, recoiling from a hot stove is a reﬂex action that is usually more successful than a slower action taken after careful deliberation.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27a1c-773d-4333-ab58-5eddb23b5746
			- All the skills needed for the Turing test also allow an agent to act rationally.
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c27a3a-531f-4e50-9f8a-d52b61552a5e
			  collapsed:: true
				- Knowledge representation and reasoning enable agents to reach good decisions.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27a8e-7097-4d7c-8bf9-aaa1c6db3492
				- We need to be able to generate comprehensible sentences in natural language to get by in a complex society.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27a9a-be71-43d8-bd66-6ffc66ac5863
				- We need learning not only for erudition, but also because it improves our ability to generate effective behavior, especially in circumstances that are new.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27aa6-a46c-445a-b96b-fb445310f851
			- The rational-agent approach to AI has two advantages over the other approaches.
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c27aca-1ff2-48d2-ac1f-8e46f9b2f324
			  collapsed:: true
				- **First**, it is more general than the “laws of thought” approach because correct inference is just one of several possible mechanisms for achieving rationality.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27aeb-8a17-4102-88ee-b4f46f7b5375
				- Second, it is more amenable to scientiﬁc development. The standard of rationality is mathematically well deﬁned and completely general.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27b14-242b-4308-b116-57f8e215299c
					- ==_However it does create duplicitous answers that lack a cohesive outcome, it is therefore fractuous. This lack of directionality and discipline towards a perceived goal, reduces certainty of action that is core to having agency. A rational agent is by definition, not an agent, but constructs the environment of potentiality. The agent must collapse the possible into the certainty of outcome based on the desirable objective. Agent as defined here exposes the agent of Chaos as it fails to act and therefore fails to exclude an infinitely large scope of undesirable outcome._==
				- We can often work back from this speciﬁcation to derive agent designs that provably achieve it—something that is largely impossible if the goal is to imitate human behavior or thought processes.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c27bd6-9bc3-42b1-8330-9b5d45e3dbad
			- For these reasons, the rational-agent approach to AI has prevailed throughout most of the ﬁeld’s history.
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c28e06-1f8a-4e61-b266-fdd43e6b4402
			  collapsed:: true
				- In the early decades, rational agents were built on logical foundations and formed deﬁnite plans to achieve speciﬁc goals.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c28e35-bfa2-48ab-90ee-f3a9476d1c91
				- Later, methods based on probability theory and machine learning allowed the creation of agents that could make decisions under uncertainty to attain the best expected outcome.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c28e46-3f56-4941-b271-3dbfe4889f10
				- __In a nutshell, AI has focused on the study and construction of agents that *do the right thing*__. What counts as the right thing is deﬁned by the objective that we provide to the agent.
				  hl-page:: 4
				  ls-type:: annotation
				  id:: 67c28e54-6a53-411d-a139-9a9fceb9d641
				  hl-color:: yellow
				- This general paradigm is so pervasive that we might call it the **standard model.** It prevails not only in AI, but also in control theory, where a controller minimizes a cost function; in operations research, where a policy maximizes a sum of rewards; in statistics, where a decision rule minimizes a loss function; and in economics, where a decision maker maximizes utility or some measure of social welfare.
				  hl-page:: 4
				  ls-type:: annotation
				  id:: 67c28ef0-5048-4f82-981d-32729c61d73b
				  hl-color:: yellow
			- We need to make one important reﬁnement to the standard model to account for the fact that perfect rationality—always taking the exactly optimal action—is not feasible in complex environments. The computational demands are just too high. 
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c2f3a2-0a0e-45d1-bef1-0e48d9ec08a9
			- Chapters 6 and 16 deal with the issue of **limited rationality**—acting appropriately when there is not enough time to do all the computations one might like. However, perfect rationality often remains a good starting point for theoretical analysis.
			  hl-stamp:: 1740829625338
			  hl-page:: 4
			  ls-type:: annotation
			  id:: 67c2f3b5-c33a-448b-8a03-43881650d963
			  hl-color:: green
		- 1.1.5 Beneﬁcial machines
		  ls-type:: annotation
		  hl-page:: 4
		  hl-color:: red
		  id:: 67c2f3df-228a-4430-927f-661b676c4437
		  collapsed:: true
			- The standard model has been a useful guide for AI research since its inception, but it is probably not the right model in the long run. 
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c2f3f0-6e05-422c-ae7e-081df0c5157a
			  collapsed:: true
				- The reason is that the standard model assumes that we will supply a fully speciﬁed objective to the machine.
				  ls-type:: annotation
				  hl-page:: 4
				  hl-color:: yellow
				  id:: 67c2f3f6-e55e-412e-9943-a0b0e49a7c0f
			- For an artiﬁcially deﬁned task such as chess or shortest-path computation, the task comes with an objective built in—so the standard model is applicable
			  ls-type:: annotation
			  hl-page:: 4
			  hl-color:: blue
			  id:: 67c2f40b-f0a5-4783-adec-2bcd464b565c
			  collapsed:: true
				- As we move into the real world, however, it becomes more and more difﬁcult to specify the objective completely and correctly.
				  hl-page:: 4
				  ls-type:: annotation
				  id:: 67c2f420-23fd-40f5-9574-3f44f5eafa80
				  hl-color:: yellow
				- For example, in designing a self-driving car, one might think that the objective is to reach the destination safely. But driving along any road incurs a risk of injury due to other errant drivers, equipment failure, and so on; thus, a strict goal of safety requires staying in the garage.
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b2c5-1c34-4615-a695-c454fe0c712b
				- There is a tradeoff between making progress towards the destination and incurring a risk of injury. How should this tradeoff be made? Furthermore, to what extent can we allow the car to take actions that would annoy other drivers? How much should the car moderate its acceleration, steering, and braking to avoid shaking up the passenger?
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b301-207e-4b81-bc6f-3d85be2e1e77
				- These kinds of questions are difﬁcult to answer a priori. They are particularly problematic in the general area of human–robot interaction, of which the self-driving car is one example.
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b309-e960-422f-a374-018af5fbd194
			- The problem of achieving agreement between our true preferences and the objective we put into the machine is called the:
			  ls-type:: annotation
			  hl-page:: 5
			  hl-color:: blue
			  id:: 67c3b31c-f086-450b-9575-56b4f3095f1d
			- **value alignment problem** - the values or objectives put into the machine must be aligned with those of the human
			  hl-page:: 5
			  ls-type:: annotation
			  id:: 67c3b33c-462f-4a4f-9e31-c874165f47e8
			  hl-color:: green
			  collapsed:: true
				- If we are developing an AI system in the lab or in a simulator—as has been the case for most of the ﬁeld’s history—there is an easyﬁx for an incorrectly speciﬁed objective: reset the system, ﬁx the objective, and try again.
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b38a-5301-4a14-a970-6de39e2b6271
				- As the ﬁeld progresses towards increasingly capable intelligent systems that are deployed in the real world, this approach is no longer viable. A system deployed with an incorrect objective will have negative consequences. Moreover, the more intelligent the system, the more negative the consequences.
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b3a7-a327-4889-871e-f7847144a14e
			- Returning to the apparently unproblematic example of chess, consider what happens if the machine is intelligent enough to reason and act beyond the conﬁnes of the chessboard.
			  ls-type:: annotation
			  hl-page:: 5
			  hl-color:: blue
			  id:: 67c3b3bf-8abc-458b-9839-0713f1c2a080
			  collapsed:: true
				- In that case, it might attempt to increase its chances of winning by such ruses as hypnotizing or blackmailing its opponent or bribing the audience to make rustling noises during its opponent’s thinking time.[^3]
				  hl-page:: 5
				  ls-type:: annotation
				  id:: 67c3b3ce-ffe7-4e97-bd27-d89e301d967e
				  hl-color:: yellow
				  collapsed:: true
					- hl-page:: 5
					  ls-type:: annotation
					  id:: 67c3b3da-d4da-4fff-8cd8-a566c775d33a
					  hl-color:: purple
					  [^3]:In one of the ﬁrst books on chess, Ruy Lopez (1561) wrote, “Always place the board so the sun is in your opponent’s eyes.”
				- It might also attempt to hijack additional computing power for itself. *These behaviors are not “unintelligent” or “insane”; they are a logical consequence of deﬁning winning as the sole objective for the machine*.
				  hl-page:: 5
				  ls-type:: annotation
				  id:: 67c3b414-dc5f-4bce-8460-d52aab715d71
				  hl-color:: yellow
			- It is impossible to anticipate all the ways in which a machine pursuing a ﬁxed objective might misbehave. There is good reason, then, to think that the standard model is inadequate.
			  hl-page:: 5
			  ls-type:: annotation
			  id:: 67c3b587-f8c0-478a-989d-22064c5087af
			  hl-color:: blue
			  collapsed:: true
				- We don’t want machines that are intelligent in the sense of pursuing *their* objectives; we want them to pursue *our* objectives. If we cannot transfer those objectives perfectly to the machine, then we need a new formulation—one in which the machine is pursuing our objectives, but is necessarily *uncertain* as to what they are.
				  hl-page:: 5
				  ls-type:: annotation
				  id:: 67c3b5c0-362f-454f-a548-5ae0abc82401
				  hl-color:: yellow
				- When a machine knows that it doesn’t know the complete objective, it has an incentive to act cautiously, to ask permission, to learn more about our preferences through observation, and to defer to human control.
				  ls-type:: annotation
				  hl-page:: 5
				  hl-color:: yellow
				  id:: 67c3b624-03d2-4c5f-a824-072afd8d503f
			- Ultimately, we want agents that are **provably beneﬁcial** to humans. We will return to this topic in Section 1.5.
			  ls-type:: annotation
			  hl-page:: 5
			  hl-color:: green
			  id:: 67c3b631-2d83-405e-993f-a37dfaf8713b
			  hl-stamp:: 1740879411869
	- 1.2 The Foundations of Artiﬁcial Intelligence
	  ls-type:: annotation
	  hl-page:: 5
	  hl-color:: red
	  id:: 67c3b65f-aee2-4c0e-87d3-1e55824895bf
	  collapsed:: true
		- In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints, and techniques to AI.
		  ls-type:: annotation
		  hl-page:: 5
		  hl-color:: blue
		  id:: 67c3b678-030c-4a4f-99b3-f82557c3e90b
		  collapsed:: true
			- Like any history, this one concentrates on a small number of people, events, and ideas and ignores others that also were important.
			  ls-type:: annotation
			  hl-page:: 5
			  hl-color:: yellow
			  id:: 67c3b693-a559-4ff0-8d08-c137059ed341
			- We organize the history around a series of questions. We certainly would not wish to give the impression that these questions are the only ones the disciplines address or that the disciplines have all been working toward AI as their ultimate fruition.
			  ls-type:: annotation
			  hl-page:: 5
			  hl-color:: yellow
			  id:: 67c3b6b3-19f7-4d4d-afa1-0d49e8fec927
		- 1.2.1 Philosophy
		  ls-type:: annotation
		  hl-page:: 6
		  hl-color:: red
		  id:: 67c3b6c1-3fdd-4671-a8ca-8cd2f9e2a1b6
		  collapsed:: true
			- [[Aristotle]] (384–322 BCE) was the ﬁrst to formulate a precise set of laws governing the rational part of the mind.
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b6d5-e144-4e8d-9b9f-309bdc11f62b
			  collapsed:: true
				- He developed an informal system of syllogisms for proper reasoning, which in principle allowed one to generate conclusions mechanically, given initial premises.
				  ls-type:: annotation
				  hl-page:: 6
				  hl-color:: yellow
				  id:: 67c3b754-4918-478f-bc97-69501ef15ff6
			- [[Ramon Llull]] (c. 1232–1315) devised a system of reasoning published as *Ars Magna* or *The Great Art* (1305). Llull tried to implement his system using an actual mechanical device: a set of paper wheels that could be rotated into different permutations.
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3b76f-6850-43be-ad0b-675e3f590861
			  hl-color:: blue
			- Around 1500, [[Leonardo da Vinci]] (1452–1519) designed but did not build a mechanical calculator; recent reconstructions have shown the design to be functional
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b7e2-91f1-4ffa-8b88-186776edb478
			- The ﬁrst known calculating machine was constructed around 1623 by the German scientist [[Wilhelm Schickard]] (1592–1635).
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b7ff-185f-4fbb-afd0-451181dbf59e
			  hl-stamp:: 1740880466711
			- [[Blaise Pascal]] (1623–1662) built the Pascaline in 1642 and wrote that it “produces effects which appear nearer to thought than all the actions of animals.
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b81a-92ff-4532-a94c-8dae08e89e36
			  hl-stamp:: 1740880485715
			- [[Gottfried Wilhelm Leibniz]] (1646–1716) built a mechanical device intended to carry out operations on concepts rather than numbers, but its scope was rather limited
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b840-f7bc-4801-ad95-5707edb0b1b4
			  hl-stamp:: 1740880491153
			- In his 1651 book *Leviathan*, [[Thomas Hobbes]] (1588–1679) suggested the idea of a thinking machine, an “artiﬁcial animal” in his words, arguing “For what is the heart but a spring; and the nerves, but so many strings; and the joints, but so many wheels.” He also suggested that reasoning was like numerical computation: “For ‘reason’ . . . is nothing but ‘reckoning,’ that is adding and subtracting.”
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3b888-7a47-4c37-8261-c93773965cbe
			  hl-stamp:: 1740880495585
			- It’s one thing to say that the mind operates, at least in part, according to logical or numerical rules, and to build physical systems that emulate some of those rules. It’s another to say that the mind itself is such a physical system.
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3b8c7-19b9-4abc-8114-0aca63b2eb28
			  hl-color:: blue
			  hl-stamp:: 1740880089028
			- [[René Descartes]] (1596–1650) gave the ﬁrst clear discussion of the distinction between mind and matter. He noted that a purely physical conception of the mind seems to leave little room for free will.
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3b8fa-ffd4-48d1-aef2-4f8faf8965f4
			  hl-color:: blue
			  hl-stamp:: 1740880460455
			  collapsed:: true
				- If the mind is governed entirely by physical laws, then it has no more free will than a rock “deciding” to fall downward.
				  ls-type:: annotation
				  hl-page:: 6
				  hl-color:: yellow
				  id:: 67c3ba26-f8da-499a-b44a-6894bd5a6d51
			- Descartes was a proponent of **dualism**. He held that there is a part of the human mind (orDualism soul or spirit) that is outside of nature, exempt from physical laws. Animals, on the other hand, did not possess this dual quality; they could be treated as machines.
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: green
			  id:: 67c3ba3c-4618-4150-b496-6be028cd638c
			- An alternative to dualism is **materialism**, which holds that the brain’s operation according to the laws of physics constitutes the mind. Free will is simply the way that the perception of available choices appears to the choosing entity.
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: green
			  id:: 67c3bb18-2f72-46d0-adf2-39a407753929
			  hl-stamp:: 1740880679166
			- The terms **physicalism** and **naturalism** are also used to describe this view that stands in contrast to the supernatural.
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3bb2f-5f4c-4060-9711-45fa920faaa3
			  hl-color:: green
			- Given a physical mind that manipulates knowledge, the next problem is to establish the source of knowledge.
			  ls-type:: annotation
			  hl-page:: 6
			  hl-color:: blue
			  id:: 67c3bb52-a6e1-49e9-ac87-d198900e6c7c
			- The **empiricism** movement, starting with [[Francis Bacon]]’s (1561–1626) *Novum Organum*,[^4] is characterized by a dictum of [[John Locke]] (1632–1704): “Nothing is in the understanding, which was not ﬁrst in the senses.”
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3bb60-ea56-45c5-b2af-5d2109b271ab
			  hl-color:: green
			  collapsed:: true
				- hl-page:: 6
				  ls-type:: annotation
				  id:: 67c3bbb0-0b43-4cbb-ad99-3f1a90c4c582
				  hl-color:: purple
				  [^4]:The *Novum Organum* is an update of Aristotle’s *Organon*, or instrument of thought.
			- [[David Hume]]’s (1711–1776) *A Treatise of Human Nature* (Hume, 1739) proposed what is now known as the principle of **induction**: that general rules are acquired by exposure toInduction repeated associations between their elements.
			  hl-page:: 6
			  ls-type:: annotation
			  id:: 67c3bfa8-92f4-4970-b441-d028aa7cc719
			  hl-color:: green
			- Building on the work of [[Ludwig Wittgenstein]] (1889–1951) and [[Bertrand Russell]] (1872–1970), the famous [[Vienna Circle]] (Sigmund, 2017), a group of philosophers and mathematicians meeting in Vienna in the 1920s and 1930s, developed the doctrine of **logical positivism**.
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: green
			  id:: 67c3bfef-8197-4675-a928-65b2ed904a0d
			- This doctrine holds that all knowledge can be characterized by logical theories connected, ultimately, to **observation sentences** that correspond to sensory inputs; thus logical positivism combines rationalism and empiricism.
			  hl-page:: 7
			  ls-type:: annotation
			  id:: 67c3c02b-33a0-4b0c-87ee-b76cbe9beafb
			  hl-color:: green
			  hl-stamp:: 1740882066321
			- The **conﬁrmation theory** of [[Rudolf Carnap]] (1891–1970) and [[Carl Hempel]] (1905–1997) attempted to analyze the acquisition of knowledge from experience by quantifying the degree of belief that should be assigned to logical sentences based on their connection to observations that conﬁrm or disconﬁrm them.
			  hl-page:: 7
			  ls-type:: annotation
			  id:: 67c3c0a6-147a-4b9e-8596-46f1c79332c0
			  hl-color:: green
			  collapsed:: true
				- Carnap’s book *The Logical Structure of the World* (1928) was perhaps the ﬁrst theory of mind as a computational process.
				  ls-type:: annotation
				  hl-page:: 7
				  hl-color:: yellow
				  id:: 67c3c0c6-6d7f-40e2-a21f-24a686c4542a
			- The ﬁnal element in the philosophical picture of the mind is the connection between knowledge and action.
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: blue
			  id:: 67c3c0e4-4b01-4887-a23d-502d7a781551
			  collapsed:: true
				- This question is vital to AI because intelligence requires action as well as reasoning. Moreover, only by understanding how actions are justiﬁed can we understand how to build an agent whose actions are justiﬁable (or rational).
				  ls-type:: annotation
				  hl-page:: 7
				  hl-color:: yellow
				  id:: 67c3c0ee-3acd-4afe-bd23-94a06eb9cd77
			- Aristotle argued (in *De Motu Animalium*) that actions are justiﬁed by a logical connection between goals and knowledge of the action’s outcome:
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: blue
			  id:: 67c3c106-0527-47f2-baf3-15e7b4d42b63
			  collapsed:: true
				- ls-type:: annotation
				  hl-page:: 7
				  hl-color:: yellow
				  id:: 67c3c27c-0022-40c1-87bc-d9359f6d0edf
				  >But how does it happen that thinking is sometimes accompanied by action and sometimes not, sometimes by motion, and sometimes not? It looks as if almost the same thing happens as in the case of reasoning and making inferences about unchanging objects. But in that case the end is a speculative proposition . . . whereas here the conclusion which results from the two premises is an action. . . . I need covering; a cloak is a covering. I need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And the conclusion, the “I have to make a cloak,” is an action.
			- In the *Nicomachean Ethics* (Book III. 3, 1112b), Aristotle further elaborates on this topic, suggesting an algorithm:
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: blue
			  id:: 67c3c2c3-19e9-4369-bb6e-dda74e78b840
			  collapsed:: true
				- hl-page:: 7
				  ls-type:: annotation
				  id:: 67c3c2d6-da7f-4993-86de-6a87416a6ccf
				  hl-color:: yellow
				  >We deliberate not about ends, but about means. For a doctor does not deliberate whether he shall heal, nor an orator whether he shall persuade, . . . They assume the end and consider how and by what means it is attained, and if it seems easily and best produced thereby; while if it is achieved by one means only they consider *how* it will be achieved by this and by what means *this* will be achieved, till they come to the ﬁrst cause, . . . and what is last in the order of analysis seems to be ﬁrst in the order of becoming. And if we come on an impossibility, we give up the search, e.g., if we need money and this cannot be got; but if a thing appears possible we try to do it.
			- [[Aristotle]]’s algorithm was implemented 2300 years later by Newell and Simon in their **General Problem Solver** program.
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: green
			  id:: 67c3c5f6-8ef5-4593-9b8e-b3f747729146
			  collapsed:: true
				- We would now call it a greedy regression planning system(see Chapter 11). Methods based on logical planning to achieve deﬁnite goals dominated theﬁrst few decades of theoretical research in AI.
				  ls-type:: annotation
				  hl-page:: 7
				  hl-color:: yellow
				  id:: 67c3c61f-10bf-4dbc-82aa-179207267a88
			- Thinking purely in terms of actions achieving goals is often useful but sometimes inapplicable.
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: blue
			  id:: 67c3db63-7d27-49b3-a20f-83907722315d
			  collapsed:: true
				- For example, if there are several different ways to achieve a goal, there needs to be some way to choose among them. More importantly, it may not be possible to achieve a goal with certainty, but some action must still be taken. How then should one decide?
				  hl-page:: 7
				  ls-type:: annotation
				  id:: 67c3dbdd-f24b-4150-950a-dd8840ce47a4
				  hl-color:: yellow
			- [[Antoine Arnauld]] (1662), analyzing the notion of rational decisions in gambling, proposed a quantitative formula for maximizing the expected monetary value of the outcome
			  ls-type:: annotation
			  hl-page:: 7
			  hl-color:: blue
			  id:: 67c3dc5b-600c-40db-803b-f54174900438
			- Later, [[Daniel Bernoulli]] (1738) introduced the more general notion of **utility** to capture the internal, subjective value of an outcome.
			  hl-page:: 7
			  ls-type:: annotation
			  id:: 67c3dc72-5201-4c31-9056-d08556979214
			  hl-color:: green
			  collapsed:: true
				- The modern notion of rational decision making under uncertainty involves maximizing expected utility, as explained in Chapter 15.
				  ls-type:: annotation
				  hl-page:: 8
				  hl-color:: yellow
				  id:: 67c3dc99-ad94-4ca5-b14b-b9f897fe7dfa
			- In matters of ethics and public policy, a decision maker must consider the interests of multiple individuals. 
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: blue
			  id:: 67c3dcb9-6f16-481f-8afb-129e09505cf6
			- [[Jeremy Bentham]] (1823) and [[John Stuart Mill]] (1863) promoted the idea of **utilitarianism**: that rational decision making based on maximizing utility should apply to all spheres of human activity, including public policy decisions made on behalf of many individuals.
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3dcd6-d54a-4c56-b2fb-cc7890ef3e71
			  hl-color:: green
			- Utilitarianism is a speciﬁc kind of **consequentialism**: the idea that what is right and wrong is determined by the expected outcomes of an action.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: green
			  id:: 67c3dd21-8cee-4e8c-a083-c92d75b7137e
			- In contrast, [[Immanuel Kant]], in 1785, proposed a theory of rule-based or **deontological ethics**, in which “doing the right thing” is determined not by outcomes but by universal social laws that govern allowable actions, such as “don’t lie” or “don’t kill.”
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3dd40-5f73-4a22-a7be-9155021905db
			  hl-color:: green
			  collapsed:: true
				- Thus, a utilitarian could tell a white lie if the expected good outweighs the bad, but a Kantian would be bound not to, because lying is inherently wrong.
				  ls-type:: annotation
				  hl-page:: 8
				  hl-color:: yellow
				  id:: 67c3dd88-92f2-4035-bcd5-a2f04f292482
				- Mill acknowledged the value of rules, but understood them as efﬁcient decision procedures compiled from ﬁrst-principles reasoning about consequences. Many modern AI systems adopt exactly this approach.
				  ls-type:: annotation
				  hl-page:: 8
				  hl-color:: yellow
				  id:: 67c3dda4-e2e5-4de7-9b5e-28396952b57c
		- 1.2.2 Mathematics
		  ls-type:: annotation
		  hl-page:: 8
		  hl-color:: red
		  id:: 67c3ddb4-e43e-49b7-a0fe-8f23a90f5afd
		  collapsed:: true
			- Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science required the mathematization of logic and probability and the introduction of a new branch of mathematics: computation.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: blue
			  id:: 67c3de63-a1c6-4724-80c6-4ccf5a899968
			- The idea of **formal logic** can be traced back to the philosophers of ancient Greece, India, and China, but its mathematical development really began with the work of [[George Boole]] (1815–1864), who worked out the details of propositional, or Boolean, logic (Boole, 1847).
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3de87-d37c-4443-9c58-82c58c41dbab
			  hl-color:: green
			- In 1879, [[Gottlob Frege]] (1848–1925) extended Boole’s logic to include objects and relations, creating the ﬁrst-order logic that is used today.[^5]
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3dedc-3265-4c66-85b7-ede4899ab774
			  hl-color:: blue
			  hl-stamp:: 1740889859943
			  collapsed:: true
				- In addition to its central role in the early period of AI research, ﬁrst-order logic motivated the work of G¨odel and Turing that underpinned computation itself, as we explain below.
				  ls-type:: annotation
				  hl-page:: 8
				  hl-color:: yellow
				  id:: 67c3df64-49cf-4557-9cdb-7284b832c1d0
				- hl-page:: 8
				  ls-type:: annotation
				  id:: 67c3deed-8775-449b-be58-2478d86c621e
				  hl-color:: purple
				  [^5]:Frege’s proposed notation for ﬁrst-order logic—an arcane combination of textual and geometric features— never became popular.
			- The theory of **probability** can be seen as generalizing logic to situations with uncertainProbability information—a consideration of great importance for AI.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: green
			  id:: 67c3df9b-d66b-458b-b313-d29c9baca24b
			- [[Gerolamo Cardano]] (1501–1576) ﬁrst framed the idea of probability, describing it in terms of the possible outcomes of gambling events.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: blue
			  id:: 67c3faf6-879d-4fae-8f6e-5d059e76228f
			- In 1654, [[Blaise Pascal]] (1623–1662), in a letter to [[Pierre Fermat]] (1601–1665), showed how to predict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3fb14-d264-49dc-8a35-1d3ce53637b9
			  hl-color:: blue
			  collapsed:: true
				- Probability quickly became an invaluable part of the quantitative sciences, helping to deal with uncertain measurements and incomplete theories
				  ls-type:: annotation
				  hl-page:: 8
				  hl-color:: yellow
				  id:: 67c3fb2c-ac24-4b7d-bae9-71864b2ddf17
			- [[Jacob Bernoulli]] (1654–1705, uncle of Daniel), [[Pierre Laplace]] (1749–1827), and others advanced the theory and introduced new statistical methods.
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3fb40-ba99-48e1-b10b-98fb304587c3
			  hl-color:: blue
			- [[Thomas Bayes]] (1702–1761) proposed a rule for updating probabilities in the light of new evidence; Bayes’ rule is a crucial tool for AI systems.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: blue
			  id:: 67c3fb61-442b-48b0-892f-ad6889c0ae9b
			- The formalization of probability, combined with the availability of data, led to the emergence of **statistics** as a ﬁeld.
			  ls-type:: annotation
			  hl-page:: 8
			  hl-color:: green
			  id:: 67c3fb76-d64e-4a0b-840c-00971e483897
			  hl-stamp:: 1740897146790
			- One of the ﬁrst uses was [[John Graunt]]’s analysis of London census data in 1662.
			  hl-page:: 8
			  ls-type:: annotation
			  id:: 67c3fb92-aefb-45e1-aa6d-190505479087
			  hl-color:: blue
			- [[Ronald Fisher]] is considered the ﬁrst modern statistician (Fisher,1922).
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: blue
			  id:: 67c3fbc7-4299-4bb6-9e33-4e2de590a486
			  collapsed:: true
				- He brought together the ideas of probability, experiment design, analysis of data, and computing—in 1919, he insisted that he couldn’t do his work without a mechanical calculator called the MILLIONAIRE (the ﬁrst calculator that could do multiplication), even though the cost of the calculator was more than his annual salary (Ross, 2012).
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c3fbdf-497c-4c66-a4b2-6884c3fb050f
			- The history of computation is as old as the history of numbers, but the ﬁrst nontrivial algorithm is thought to be Euclid’s **algorithm** for computing greatest common divisors.
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: green
			  id:: 67c3fbf3-2240-443b-ab30-871e8c1292a9
			- The word algorithm comes from [[Muhammad ibn Musa al-Khwarizmi]], a 9th century mathematician, whose writings also introduced Arabic numerals and algebra to Europe.
			  hl-page:: 9
			  ls-type:: annotation
			  id:: 67c3fc0b-de53-4270-95d6-670f188d4e3f
			  hl-color:: blue
			  collapsed:: true
				- Boole and others discussed algorithms for logical deduction, and, by the late 19th century, efforts were under way to formalize general mathematical reasoning as logical deduction.
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c3fc21-1be3-4ab2-930c-776c4d4cf583
			- [[Kurt Gödel]] (1906–1978) showed that there exists an effective procedure to prove any true statement in the ﬁrst-order logic of Frege and Russell, but that ﬁrst-order logic could not capture the principle of mathematical induction needed to characterize the natural numbers. In1931, Gödel showed that limits on deduction do exist.
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: blue
			  id:: 67c3fca2-987a-435a-8ca7-e6812e2b9551
			  hl-stamp:: 1740897480720
			- His **incompleteness theorem** showed that in any formal theory as strong as Peano arithmetic (the elementary theory of natural numbers), there are necessarily true statements that have no proof within the theory.
			  hl-page:: 9
			  ls-type:: annotation
			  id:: 67c3fce7-bc9e-4202-862d-4da832751bd6
			  hl-color:: green
			- This fundamental result can also be interpreted as showing that some functions on the integers cannot be represented by an algorithm—that is, they cannot be computed.
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: blue
			  id:: 67c3fd17-fc44-494a-9060-9825dd997c6a
			- This motivated [[Alan Turing]] (1912–1954) to try to characterize exactly which functions are **computable**—capable of being computed by an effective procedure.
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: green
			  id:: 67c400e1-2f9a-4e50-b2b2-b354b1f98722
			  collapsed:: true
				- The Church–Turing thesis proposes to identify the general notion of computability with functions computed by a Turing machine (Turing, 1936). Turing also showed that there were some functions that no Turing machine can compute. 
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c4011b-b9a7-4d67-8ea1-1328d5c90a02
				- For example, no machine can tell *in general* whether a given program will return an answer on a given input or run forever.
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c40125-8104-414b-b666-5964fdea2155
			- Although computability is important to an understanding of computation, the notion of **tractability** has had an even greater impact on AI.
			  ls-type:: annotation
			  hl-page:: 9
			  hl-color:: green
			  id:: 67c40155-883b-4e94-a2ba-8dced243d3a3
			  collapsed:: true
				- Roughly speaking, a problem is called intractable if the time required to solve instances of the problem grows exponentially with the size of the instances.
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c4016e-7828-4241-baae-960ae44c34e8
				- The distinction between polynomial and exponential growth in complexity was ﬁrst emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965).
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c4018b-d50a-45bc-8d78-ced39ee802fb
				- It is important because exponential growth means that even moderately large instances cannot be solved in any reasonable time.
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c4019c-5274-4cfd-b323-9ead9909b271
			- The theory of **NP-completeness**, pioneered by Cook (1971) and Karp (1972), provides a basis for analyzing the tractability of problems: any problem class to which the class of NPcomplete problems can be reduced is likely to be intractable. (Although it has not been proved that NP-complete problems are necessarily intractable, most theoreticians believe it.)
			  hl-page:: 9
			  ls-type:: annotation
			  id:: 67c401ba-44b5-498a-843b-49e9f6f2b748
			  hl-color:: blue
			  collapsed:: true
				- These results contrast with the optimism with which the popular press greeted the ﬁrst computers—“Electronic Super-Brains” that were “Faster than Einstein!” 
				  ls-type:: annotation
				  hl-page:: 9
				  hl-color:: yellow
				  id:: 67c401d9-2795-4a0c-888f-83182a4f3724
				- Despite the increasing speed of computers, careful use of resources and necessary imperfection will characterize intelligent systems. Put crudely, the world is an *extremely* large problem instance!
				  hl-page:: 9
				  ls-type:: annotation
				  id:: 67c401e8-4318-4498-b8fa-b10fbd31a4ba
				  hl-color:: yellow
		- 1.2.3 Economics
		  ls-type:: annotation
		  hl-page:: 9
		  hl-color:: red
		  id:: 67c401fc-8b36-4c23-bc15-ea3622b4a494
		  collapsed:: true
			- The science of economics originated in 1776, when Adam Smith (1723–1790) published *An Inquiry into the Nature and Causes of the Wealth of Nations*.
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: blue
			  id:: 67c40215-9175-4f0d-bbcd-d071f47bae13
			  collapsed:: true
				- Smith proposed to analyze economies as consisting of many individual agents attending to their own interests
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c40247-d299-4906-9dc9-cccdb96ccd7e
				- Smith was not, however, advocating ﬁnancial greed as a moral position: his earlier (1759) book *The Theory of Moral Sentiments* begins by pointing out that concern for the well-being of others is an essential component of the interests of every individual.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c4026d-e12a-48cf-91db-eb086a2af6c6
			- Most people think of economics as being about money, and indeed the ﬁrst mathematical analysis of decisions under uncertainty, the maximum-expected-value formula of Arnauld(1662), dealt with the monetary value of bets
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: blue
			  id:: 67c4029a-9a38-4071-8633-74a5088a197f
			- [[Daniel Bernoulli]] (1738) noticed that this formula didn’t seem to work well for larger amounts of money, such as investments in maritime trading expeditions.
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: blue
			  id:: 67c402af-82f7-48c6-89e9-0e1c74d07b38
			  hl-stamp:: 1740898995625
			  collapsed:: true
				- He proposed instead a principle based on maximization of expected utility, and explained human investment choices by proposing that the marginal utility of an additional quantity of money diminished as one acquired more money.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c402cc-b798-4fef-8194-eba8427dbd2d
			- [[Léon Walras]] (pronounced “Valrasse”) (1834–1910) gave utility theory a more general foundation in terms of preferences between gambles on any outcomes (not just monetary outcomes).
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c40301-4e67-4d03-b71f-df4349ad5f1d
			  hl-color:: blue
			  collapsed:: true
				- The theory was improved by Ramsey (1931) and later by [[John von Neumann]] and [[Oskar Morgenstern]] in their book The Theory of Games and Economic Behavior (1944).
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c4038b-efcb-4be0-9133-cb47a6bb260b
				- Economics is no longer the study of money; rather it is the study of desires and preferences.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c403bc-5d3d-4034-a8f8-24360bb21378
			- **Decision theory**, which combines probability theory with utility theory, provides a formal and complete framework for individual decisions (economic or otherwise) made under uncertainty—that is, in cases where probabilistic descriptions appropriately capture the decision maker’s environment.
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c403f3-c3df-4bda-83ea-3352fbfd5ac3
			  hl-color:: green
			  collapsed:: true
				- This is suitable for “large” economies where each agent need pay no attention to the actions of other agents as individuals
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c4041a-69e0-4936-b7e8-f7bb586c26a2
				- For “small” economies, the situation is much more like a **game**: the actions of one player can signiﬁcantly affect the utility of another (either positively or negatively).
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c40431-5cf1-4676-8a09-a5af9dc2055c
				  hl-stamp:: 1740899448523
			- [[John von Neumann]] and [[Oskar Morgenstern]]’s development of **game theory** (see also Luce and Raiffa, 1957) included the surprising result that, for some games, a rational agent should adopt policies that are (or least appear to be) randomized.
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c40444-d00f-4b27-8b4b-8e295967da45
			  hl-color:: green
			  hl-stamp:: 1740899443569
			  collapsed:: true
				- Unlike decision theory, game theory does not offer an unambiguous prescription for selecting actions.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c4049d-1585-4c27-be81-3bee7d56be58
			- In AI, decisions involving multiple agents are studied under the heading of **multiagent systems** (Chapter 17).
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: green
			  id:: 67c404a9-d386-4ced-aaa2-b1874f63da36
			- Economists, with some exceptions, did not address the third question listed above: how to make rational decisions when payoffs from actions are not immediate but instead result from several actions taken *in sequence*.
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: blue
			  id:: 67c404c7-bd34-4d6c-a9fd-c7b13d84de4d
			- This topic was pursued in the ﬁeld of **operations research**, which emerged in World War II from efforts in Britain to optimize radar installations, and later found innumerable civilian applications.
			  hl-stamp:: 1740899556482
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c404de-9f1d-440d-9052-d4c75fbb3bf8
			  hl-color:: green
			- The work of [[Richard Bellman]] (1957) formalized a class of sequential decision problems called **Markov decision processes**, which we study in Chapter 16 and, under the heading of **reinforcement learning**, in Chapter 23.
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c40515-71d1-4860-b59c-6b9f419cf8f3
			  hl-color:: green
			- Work in economics and operations research has contributed much to our notion of rational agents, yet for many years AI research developed along entirely separate paths.
			  ls-type:: annotation
			  hl-page:: 10
			  hl-color:: blue
			  id:: 67c4053d-2504-4d50-ace7-6ced9125b48f
			  collapsed:: true
				- One reason was the apparent complexity of making rational decisions.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c40563-a635-4b18-91dc-a14ee5042be4
			- The pioneering AI researcher [[Herbert Simon]] (1916–2001) won the Nobel Prize in economics in 1978 for his early work showing that models based on **satisﬁcing**—making decisions that are “good enough,” rather than laboriously calculating an optimal decision—gave a better description of actual human behavior (Simon, 1947).
			  hl-page:: 10
			  ls-type:: annotation
			  id:: 67c40574-5a9f-4685-bd28-77b3ca1829c2
			  hl-color:: blue
			  collapsed:: true
				- Since the 1990s, there has been a resurgence of interest in decisiontheoretic techniques for AI.
				  ls-type:: annotation
				  hl-page:: 10
				  hl-color:: yellow
				  id:: 67c405a3-3123-4f8e-b76c-b0c1eea67475
		- 1.2.4 Neuroscience
		  ls-type:: annotation
		  hl-page:: 11
		  hl-color:: red
		  id:: 67c405b4-577c-4366-b309-e105b27e6d86
		  collapsed:: true
			- **Neuroscience** is the study of the nervous system, particularly the brain.
			  ls-type:: annotation
			  hl-page:: 11
			  hl-color:: blue
			  id:: 67c405d0-d9a0-4116-bf57-52b06eedaa69
			  collapsed:: true
				- Although the exact way in which the brain enables thought is one of the great mysteries of science, the fact that it does enable thought has been appreciated for thousands of years because of the evidence that strong blows to the head can lead to mental incapacitation.
				  ls-type:: annotation
				  hl-page:: 11
				  hl-color:: yellow
				  id:: 67c405ee-56a8-4688-8e07-e177560d06a2
				- It has also long been known that human brains are somehow different; in about 335 BCE Aristotle wrote, “Of all the animals, man has the largest brain in proportion to his size.”[^6]
				  hl-page:: 11
				  ls-type:: annotation
				  id:: 67c4060a-96e3-448b-842f-e70eec1b03d4
				  hl-color:: yellow
					- hl-page:: 11
					  ls-type:: annotation
					  id:: 67c4061d-0a3f-4172-a933-1252532db234
					  hl-color:: purple
					  [^6]:It has since been discovered that the tree shrew and some bird species exceed the human brain/body ratio.
				- Still, it was not until the middle of the18th century that the brain was widely recognized as the seat of consciousness. Before then, candidate locations included the heart and the spleen.
				  ls-type:: annotation
				  hl-page:: 11
				  hl-color:: yellow
				  id:: 67c4063b-a24e-4b4e-96e7-a304311412e4
			- [[Paul Broca]]’s (1824–1880) investigation of aphasia (speech deﬁcit) in brain-damaged patients in 1861 initiated the study of the brain’s functional organization by identifying a localized area in the left hemisphere—now called Broca’s area—that is responsible for speech production.[^7]
			  hl-page:: 11
			  ls-type:: annotation
			  id:: 67c40659-3bd3-45e2-ab74-92cb6a998783
			  hl-color:: blue
			  collapsed:: true
				- hl-page:: 11
				  ls-type:: annotation
				  id:: 67c40673-419a-422e-906b-4e64b2ced7ba
				  hl-color:: purple
				  [^7]:Many cite Alexander Hood (1824) as a possible prior source.
			- By that time, it was known that the brain consisted largely of nerve cells, or **neurons**, but it was not until 1873 that [[Camillo Golgi]] (1843–1926) developed a staining technique allowing the observation of individual neurons (see **Figure 1.1**).
			  hl-stamp:: 1740900025560
			  hl-page:: 11
			  ls-type:: annotation
			  id:: 67c406b6-b014-48a5-907d-52dc85fa8345
			  hl-color:: green
			  collapsed:: true
				- [:span]
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c406e7-aa83-46d7-b897-9ce5cb770eb0
				  hl-type:: area
				  hl-stamp:: 1740900070394
			- Brains and digital computers have somewhat different properties. **Figure 1.2** shows that computers have a cycle time that is a million times faster than a brain.
			  ls-type:: annotation
			  hl-page:: 12
			  hl-color:: blue
			  id:: 67c4071f-6618-49db-9329-dda117afea20
			  collapsed:: true
				- The brain makes up for that with far more storage and interconnection than even a high-end personal computer, although the largest supercomputers match the brain on some metrics.
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c40756-bd74-47f1-913b-23053a20e96c
				- [:span]
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c4073d-e8af-4963-ba45-6be4b8a473f5
				  hl-type:: area
				  hl-stamp:: 1740900156069
			- Futurists make much of these numbers, pointing to an approaching singularity at which computers reach a su-Singularity perhuman level of performance (Vinge, 1993; Kurzweil, 2005; Doctorow and Stross, 2012), and then rapidly improve themselves even further. 
			  ls-type:: annotation
			  hl-page:: 12
			  hl-color:: green
			  id:: 67c40765-4797-45ff-b42d-f909d16ee53c
			  collapsed:: true
				- But the comparisons of raw numbers are not especially informative. Even with a computer of virtually unlimited capacity, we still require further conceptual breakthroughs in our understanding of intelligence (see Chapter 29).
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c40784-76fe-4fc8-bb52-339858a6b304
				- Crudely put, without the right theory, faster machines just give you the wrong answer faster
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c4078e-81b8-4ecd-a023-c66d283b3237
		- 1.2.5 Psychology
		  ls-type:: annotation
		  hl-page:: 12
		  hl-color:: red
		  id:: 67c40797-ec2f-4a02-b561-ba18b3a0ae2b
		  collapsed:: true
			- The origins of scientiﬁc psychology are usually traced to the work of the German physicist [[Hermann von Helmholtz]] (1821–1894) and his student [[Wilhelm Wundt]] (1832–1920).
			  ls-type:: annotation
			  hl-page:: 12
			  hl-color:: blue
			  id:: 67c407c1-3eb2-4a90-97af-89b643ac34cb
			  collapsed:: true
				- Helmholtz applied the scientiﬁc method to the study of human vision, and his *Handbook of Physiological Optics* has been described as “the single most important treatise on the physics and physiology of human vision” (Nalwa, 1993, p.15).
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c407e5-5db5-4201-a02d-f3241fb907f3
				- In 1879, Wundt opened the ﬁrst laboratory of experimental psychology, at the University of Leipzig.
				  ls-type:: annotation
				  hl-page:: 12
				  hl-color:: yellow
				  id:: 67c40803-bb97-497e-88b1-25ef7434827e
				- Wundt insisted on carefully controlled experiments in which his workers would perform a perceptual or associative task while introspecting on their thought processes.
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c40819-d7e8-44f3-9a18-312b76f5318b
				- The careful controls went a long way toward making psychology a science, but the subjective nature of the data made it unlikely that experimenters would ever disconﬁrm their own theories.
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c4082f-674a-49b0-86d6-39f3ed316996
			- Biologists studying animal behavior, on the other hand, lacked introspective data and developed an objective methodology, as described by [[H. S. Jennings]] (1906) in his inﬂuential work *Behavior of the Lower Organisms*.
			  hl-page:: 13
			  ls-type:: annotation
			  id:: 67c40856-b14d-4245-8c73-165d3649f2fa
			  hl-color:: blue
			- Applying this viewpoint to humans, the **behaviorism** movement, led by [[John Watson]] (1878–1958), rejected any theory involving mental processes on the grounds that introspection could not provide reliable evidence.
			  hl-page:: 13
			  ls-type:: annotation
			  id:: 67c40878-6fd9-46bb-90b2-5a0beb3df77a
			  hl-color:: green
			  collapsed:: true
				- Behaviorists insisted on studying only objective measures of the percepts (or *stimulus*) given to an animal and its resulting actions (or *response*).
				  hl-page:: 13
				  ls-type:: annotation
				  id:: 67c408a2-5914-428e-aeb1-0d094fb9d787
				  hl-color:: yellow
				- Behaviorism discovered a lot about rats and pigeons but had less success at understanding humans.
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c408b8-b294-4281-8b3d-4a4a118a60a9
			- **Cognitive psychology**, which views the brain as an information-processing device, can be traced back at least to the works of [[William James]] (1842–1910).
			  hl-page:: 13
			  ls-type:: annotation
			  id:: 67c408cf-e02c-4e7d-b85e-19159f636363
			  hl-color:: green
			  collapsed:: true
				- Helmholtz also insisted that perception involved a form of unconscious logical inference.
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c408fd-2c93-45dd-bd78-db69881a976a
			- The cognitive viewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Applied Psychology Unit, directed by [[Frederic Bartlett]] (1886–1969), cognitive modeling was able to ﬂourish.
			  ls-type:: annotation
			  hl-page:: 13
			  hl-color:: blue
			  id:: 67c4090f-9795-456d-aa2e-74b5384a6554
			  collapsed:: true
				- *The Nature of Explanation*, by Bartlett’s student and successor [[Kenneth Craik]] (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and goals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talk about gases, despite gasses being made of molecules that have neither.
				  hl-page:: 13
				  ls-type:: annotation
				  id:: 67c40932-4ab5-4cc2-9826-b1256086277e
				  hl-color:: yellow
			- Craik speciﬁed the three key steps of a knowledge-based agent
			  ls-type:: annotation
			  hl-page:: 13
			  hl-color:: blue
			  id:: 67c40969-7b64-477d-89a4-2646af1f6989
			  collapsed:: true
				- (1) the stimulus must be translated into an internal representation,
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c4096e-3549-4e02-8c0b-46a352b8ddce
				- (2) the representation is manipulated by cognitive processes to derive new internal representations, and
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c40977-4bfb-4dd6-b2f8-1dd081ecff8d
				- (3) these are in turn retranslated back into action. He clearly explained why this was a good design for an agent:
				  ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c40980-7102-42df-8956-c4d56f878234
				- ls-type:: annotation
				  hl-page:: 13
				  hl-color:: yellow
				  id:: 67c409b2-fd80-416f-938b-3c6bc8fa33cf
				  >If the organism carries a “small-scale model” of external reality and of its own possible actions within its head, it is able to try out various alternatives, conclude which is the best of them, react to future situations before they arise, utilize the knowledge of past events in dealing with the present and future, and in every way to react in a much fuller, safer, and more competent manner to the emergencies which face it. (Craik, 1943)
			- After Craik’s death in a bicycle accident in 1945, his work was continued by [[Donald Broadbent]], whose book *Perception and Communication* (1958) was one of the ﬁrst works to model psychological phenomena as information processing.
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40b2a-9884-49ea-8f75-271b6bcf5093
			  hl-color:: blue
			- Meanwhile, in the United States, the development of computer modeling led to the creation of the ﬁeld of **cognitive science**.
			  ls-type:: annotation
			  hl-page:: 14
			  hl-color:: green
			  id:: 67c40b53-e292-4ae8-9025-137e0a92cffa
			  collapsed:: true
				- Theﬁeld can be said to have started at a workshop in September 1956 at MIT—just two months after the conference at which AI itself was “born.”
				  ls-type:: annotation
				  hl-page:: 14
				  hl-color:: yellow
				  id:: 67c40b61-c28e-4120-b92c-3058730d511d
			- At the workshop, George Miller presented *The Magic Number Seven*, Noam Chomsky presented *Three Models of Language*, and Allen Newell and Herbert Simon presented *The Logic Theory Machine*.
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40bce-8b95-400c-b55a-dae0844190f3
			  hl-color:: blue
			  collapsed:: true
				- These three inﬂuential papers showed how computer models could be used to address the psychology of memory, language, and logical thinking, respectively.
				  ls-type:: annotation
				  hl-page:: 14
				  hl-color:: yellow
				  id:: 67c40c05-750e-4904-a18e-34270cd4eafe
				- It is now a common (although far from universal) view among psychologists that “a cognitive theory should be like a computer program” (Anderson, 1980); that is, it should describe the operation of a cognitive function in terms of the processing of information.
				  ls-type:: annotation
				  hl-page:: 14
				  hl-color:: yellow
				  id:: 67c40c26-762c-4d73-8d74-4b85ce5591d0
			- For purposes of this review, we will count the ﬁeld of **human–computer interaction** (HCI) under psychology.
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40c37-112e-4a83-8735-ff35da9caa9d
			  hl-color:: green
			- [[Doug Engelbart]], one of the pioneers of HCI, championed the idea of **intelligence augmentation**—IA rather than AI.
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40c4d-6632-4a89-b0dd-1864b08b7c0d
			  hl-color:: green
			  collapsed:: true
				- He believed that computers should augment human abilities rather than automate away human tasks.
				  hl-page:: 14
				  ls-type:: annotation
				  id:: 67c40c8c-ab4b-4e64-a941-cbdbbe25fdbc
				  hl-color:: yellow
				- In 1968, Engelbart’s “mother of all demos” showed off for the ﬁrst time the computer mouse, a windowing system, hypertext, and video conferencing—all in an effort to demonstrate what human knowledge workers could collectively accomplish with some intelligence augmentation.
				  ls-type:: annotation
				  hl-page:: 14
				  hl-color:: yellow
				  id:: 67c40cae-0115-424f-a615-2828572e95c6
			- Today we are more likely to see IA and AI as two sides of the same coin, with the former emphasizing human control and the latter emphasizing intelligent behavior on the part of the machine. Both are needed for machines to be useful to humans.
			  ls-type:: annotation
			  hl-page:: 14
			  hl-color:: blue
			  id:: 67c40d31-6f02-4c3d-ade7-558024d5a57d
		- 1.2.6 Computer engineering
		  ls-type:: annotation
		  hl-page:: 14
		  hl-color:: red
		  id:: 67c40d54-a4d4-4053-983d-087c00a19ec2
		  collapsed:: true
			- The modern digital electronic computer was invented independently and almost simultaneously by scientists in three countries embattled in World War II.
			  ls-type:: annotation
			  hl-page:: 14
			  hl-color:: blue
			  id:: 67c40de9-188b-4c21-a192-5694a363e916
			  collapsed:: true
				- The ﬁrst *operational* computer was the electromechanical Heath Robinson,[^9] built in 1943 by Alan Turing’s team for a single purpose: deciphering German messages.
				  hl-page:: 14
				  ls-type:: annotation
				  id:: 67c40e0a-ee72-4345-b4d3-f8aab19123e4
				  hl-color:: yellow
					- hl-page:: 14
					  ls-type:: annotation
					  id:: 67c40eb9-217c-4b69-91c8-e8394a7c9db7
					  hl-color:: purple
					  [^9]:A complex machine named after a British cartoonist who depicted whimsical and absurdly complicated contraptions for everyday tasks such as buttering toast.
				- In 1943, the same group developed the Colossus, a powerful general-purpose machine based on vacuum tubes.[^10]
				  hl-page:: 14
				  ls-type:: annotation
				  id:: 67c40ea7-2398-4777-a14e-7f1c442e97da
				  hl-color:: yellow
					- hl-page:: 14
					  ls-type:: annotation
					  id:: 67c40ed2-8189-4166-9ed7-8bdeda5c76c9
					  hl-color:: purple
					  [^10]:In the postwar period, Turing wanted to use these computers for AI research—for example, he created an outline of the ﬁrst chess program (Turing et al., 1953)—but the British government blocked this research.
			- The ﬁrst operational *programmable* computer was the Z-3, the invention of [[Konrad Zuse]] in Germany in1941.
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40eeb-07fd-460a-9530-c249578caaa1
			  hl-color:: blue
			  collapsed:: true
				- Zuse also invented ﬂoating-point numbers and the ﬁrst high-level programming language, Plankalkül.
				  hl-page:: 14
				  ls-type:: annotation
				  id:: 67c40f24-52a4-4621-8169-a812505a9080
				  hl-color:: yellow
			- The ﬁrst *electronic* computer, the ABC, was assembled by [[John Atanasoff]] and his student [[Clifford Berry]] between 1940 and 1942 at Iowa State University
			  hl-page:: 14
			  ls-type:: annotation
			  id:: 67c40f70-a6ab-4982-8132-a790a86f000c
			  hl-color:: blue
			  hl-stamp:: 1740902307036
			  collapsed:: true
				- Atanasoff’s research received little support or recognition; it was the ENIAC, developed as part of a secret military project at the University of Pennsylvania by a team including John Mauchly and J. Presper Eckert, that proved to be the most inﬂuential forerunner of modern computers.
				  ls-type:: annotation
				  hl-page:: 14
				  hl-color:: yellow
				  id:: 67c40f9f-e3e7-44e3-8275-dec1c4bffe06
			- Since that time, each generation of computer hardware has brought an increase in speed and capacity and a decrease in price—a trend captured in **Moore’s law**.
			  ls-type:: annotation
			  hl-page:: 14
			  hl-color:: green
			  id:: 67c40fc7-4328-4140-b108-20c159d09739
			  collapsed:: true
				- Performance doubled every 18 months or so until around 2005, when power dissipation problems led manufacturers to start multiplying the number of CPU cores rather than the clock speed.
				  hl-page:: 14
				  ls-type:: annotation
				  id:: 67c40fda-d2d1-4c6c-a815-10fc92ecf380
				  hl-color:: yellow
				- Current expectations are that future increases in functionality will come from massive parallelism—a curious convergence with the properties of the brain. 
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c4100c-0344-4821-86e1-1189bf497468
				- We also see new hardware designs based on the idea that in dealing with an uncertain world, we don’t need 64 bits of precision in our numbers; just 16 bits (as in the bfloat16 format) or even 8 bits will be enough, and will enable faster processing.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c41069-3321-4a79-992e-3172024759ec
			- We are just beginning to see hardware tuned for AI applications, such as the graphics processing unit (GPU), tensor processing unit (TPU), and wafer scale engine (WSE).
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c41089-2ca8-414d-b3c1-f044f132e095
			  collapsed:: true
				- From the 1960s to about 2012, the amount of computing power used to train top machine learning applications followed Moore’s law.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c410e9-8ea3-400e-83ec-336c7c64a252
				- Beginning in 2012, things changed: from 2012 to2018 there was a 300,000-fold increase, which works out to a doubling every 100 days or so (Amodei and Hernandez, 2018).
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c41109-831a-4323-8b22-3129672164f1
				- A machine learning model that took a full day to train in 2014 takes only two minutes in 2018 (Ying et al., 2018).
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c4112d-f434-47ca-b043-28c0ebb3123d
			- Although it is not yet practical, **quantum computing** holds out the promise of far greater accelerations for some important subclasses of AI algorithms.
			  hl-page:: 15
			  ls-type:: annotation
			  id:: 67c41137-0d95-4356-8a96-fd614f44447e
			  hl-color:: green
			- Of course, there were calculating devices before the electronic computer. The earliest automated machines, dating from the 17th century, were discussed on page 24.
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c41179-0017-4ebd-9511-ee80634770fe
			- The ﬁrst programmable machine was a loom, devised in 1805 by [[Joseph Marie Jacquard]] (1752–1834), that used punched cards to store instructions for the pattern to be woven.
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c4118e-37d0-4b63-95dc-7bf419753012
			- In the mid-19th century, [[Charles Babbage]] (1792–1871) designed two computing machines, neither of which he completed.
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c411b4-c33d-4ac1-a709-5fca71ed4bac
			  collapsed:: true
				- The Difference Engine was intended to compute mathematical tables for engineering and scientiﬁc projects. It was ﬁnally built and shown to work in 1991 (Swade, 2000).
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c411f8-7c4e-4012-a8f0-a38720fa3f83
				- Babbage’s Analytical Engine was far more ambitious: it included addressable memory, stored programs based on Jacquard’s punched cards, and conditional jumps. It was the ﬁrst machine capable of universal computation.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c41208-dddd-4154-b263-f10a15ef09bf
			- Babbage’s colleague [[Ada Lovelace]], daughter of the poet [[Lord Byron]], understood its potential, describing it as “a thinking or . . . a reasoning machine,” one capable of reasoning about “all subjects in the universe” (Lovelace, 1843).
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c41239-6867-4c39-8c0d-fb4e2ee5b865
			  collapsed:: true
				- She also anticipated AI’s hype cycles, writing, “It is desirable to guard against the possibility of exaggerated ideas that might arise as to the powers of the Analytical Engine.” Unfortunately, Babbage’s machines and Lovelace’s ideas were largely forgotten.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c41262-d870-405a-aec1-5c0985bf657c
			- AI also owes a debt to the software side of computer science, which has supplied the operating systems, programming languages, and tools needed to write modern programs (and papers about them)
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c412d2-055b-418f-a11f-e8e36c8ab958
			  collapsed:: true
				- But this is one area where the debt has been repaid: work in AI has pioneered many ideas that have made their way back to mainstream computer science, including time sharing, interactive interpreters, personal computers with windows and mice, rapid development environments, the linked-list data type, automatic storage management, and key concepts of symbolic, functional, declarative, and object-oriented programming.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c412f4-389a-4c00-8eba-29d8e9681411
		- 1.2.7 Control theory and cybernetics
		  ls-type:: annotation
		  hl-page:: 15
		  hl-color:: red
		  id:: 67c41313-10e0-48d1-9240-7d8bfc5e485a
		  collapsed:: true
			- [[Ktesibios of Alexandria]] (c. 250 BCE) built the ﬁrst self-controlling machine: a water clock with a regulator that maintained a constant ﬂow rate.
			  ls-type:: annotation
			  hl-page:: 15
			  hl-color:: blue
			  id:: 67c41341-7e39-4108-8e98-ebcdbff85dd1
			  collapsed:: true
				- This invention changed the deﬁnition of what an artifact could do. Previously, only living things could modify their behavior in response to changes in the environment.
				  ls-type:: annotation
				  hl-page:: 15
				  hl-color:: yellow
				  id:: 67c41415-ef6b-411e-9be0-b7bd5ce3215a
			- Other examples of self-regulating feedback control systems include the steam engine governor, created by [[James Watt]] (1736–1819), and the thermostat, invented by [[Cornelis Drebbel]] (1572–1633), who also invented the submarine.
			  hl-page:: 15
			  ls-type:: annotation
			  id:: 67c4142d-2bf2-4d57-9d4f-546d8cdbfabb
			  hl-color:: blue
			- [[James Clerk Maxwell]] (1868) initiated the mathematical theory of control systems.
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: blue
			  id:: 67c4145f-13e6-4595-9fc3-6c12dad77487
			  hl-stamp:: 1740903595765
			- A central ﬁgure in the post-war development of **control theory** was [[Norbert Wiener]] (1894–1964).
			  hl-page:: 16
			  ls-type:: annotation
			  id:: 67c41489-45a9-43c3-ac19-358ff55a73a1
			  hl-color:: green
			  hl-stamp:: 1740903768804
			- Wiener was a brilliant mathematician who worked with [[Bertrand Russell,]] among others, before developing an interest in biological and mechanical control systems and their connection to cognition.
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: blue
			  id:: 67c414c1-9ebd-4053-a013-97113d14b433
			- Like [[Kenneth Craik]] (who also used control systems as psychological models), [[Norbert Wiener]] and his colleagues [[Arturo Rosenblueth]] and [[Julian Bigelow]] challenged the behaviorist orthodoxy (Rosenblueth et al., 1943).
			  hl-page:: 16
			  ls-type:: annotation
			  id:: 67c41538-7c22-4551-8c4a-315c5d165be4
			  hl-color:: blue
			  collapsed:: true
				- They viewed purposive behavior as arising from a regulatory mechanism trying to minimize “error”—the difference between current state and goal state.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41599-4427-465d-a0c1-fb89940480bf
			- In the late 1940s, [[Norbert Wiener]] , along with [[Warren McCulloch]], [[Walter Pitts]], and [[John von Neumann]], organized a series of inﬂuential conferences that explored the new mathematical and computational models of cognition.
			  hl-page:: 16
			  ls-type:: annotation
			  id:: 67c415c8-3446-4db1-957f-8f02c469cb70
			  hl-color:: blue
			  collapsed:: true
				- Wiener’s book *Cybernetics* (1948) Cybernetics became a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.
				  hl-page:: 16
				  ls-type:: annotation
				  id:: 67c4160b-79f0-4279-8ec7-6607fdc94341
				  hl-color:: yellow
			- Meanwhile, in Britain, [[W. Ross Ashby]] pioneered similar ideas (Ashby, 1940).
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: blue
			  id:: 67c4165b-3bee-4d10-88a3-e6f4c56648c7
			  collapsed:: true
				- Ashby, [[Alan Turing]], [[Grey Walter]], and others formed the Ratio Club for “those who had Wiener’s ideas before Wiener’s book appeared.”
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41675-0714-4069-92b5-fcbc9342904c
			- [[W. Ross Ashby]]’s *Design for a Brain* (1948, 1952) elaborated on his idea that intelligence could be created by the use of **homeostatic** devices containing appropriate feedback loops to achieve stable adaptive behavior.
			  hl-page:: 16
			  ls-type:: annotation
			  id:: 67c41697-6def-423d-adba-159b6767759f
			  hl-color:: green
			- Modern control theory, especially the branch known as stochastic optimal control, has as its goal the design of systems that minimize a **cost function** over time.
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: green
			  id:: 67c416e6-639a-4a96-b09e-4e88f441b47c
			  collapsed:: true
				- This roughly matchesCost function the standard model of AI: designing systems that behave optimally.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41708-0092-4948-9961-f53481f790c6
				- Why, then, are AI and control theory two different ﬁelds, despite the close connections among their founders?
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41718-e0e7-46d3-aa03-6d49a3836916
				- The answer lies in the close coupling between the mathematical techniques that were familiar to the participants and the corresponding sets of problems that were encompassed in each world view. 
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41729-6f41-4339-945d-ae64616f08c3
				- Calculus and matrix algebra, the tools of control theory, lend themselves to systems that are describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way to escape from these perceived limitations.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c4173e-d644-4ccd-bf0c-fd4c91926628
				- The tools of logical inference and computation allowed AI researchers to consider problems such as language, vision, and symbolic planning that fell completely outside the control theorist’s purview.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c4174e-faaf-4b73-b59d-fddb4578c850
		- 1.2.8 Linguistics
		  ls-type:: annotation
		  hl-page:: 16
		  hl-color:: red
		  id:: 67c41764-2019-4a45-a42f-62835fd650f2
		  collapsed:: true
			- In 1957, [[B. F. Skinner]] published *Verbal Behavior*. This was a comprehensive, detailed account of the behaviorist approach to language learning, written by the foremost expert in the ﬁeld.
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: blue
			  id:: 67c41788-01f7-4287-b6b9-265617a2cf5b
			  collapsed:: true
				- But curiously, a review of the book became as well known as the book itself, and served to almost kill off interest in behaviorism.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c417c6-a46e-42f3-b5ba-94abe2f30adf
			- The author of the review was the linguist [[Noam Chomsky]], who had just published a book on his own theory, *Syntactic Structures*.
			  ls-type:: annotation
			  hl-page:: 16
			  hl-color:: blue
			  id:: 67c417d9-a704-4497-9d2f-694072b10e41
			  collapsed:: true
				- Chomsky pointed out that the behaviorist theory did not address the notion of creativity in language—it did not explain how children could understand and make up sentences that they had never heard before.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c417f9-e957-45b0-843f-2a95b37040e2
				- Chomsky’s theory—based on syntactic models going back to the Indian linguist Panini (c. 350 BCE)—could explain this, and unlike previous theories, it was formal enough that it could in principle be programmed.
				  ls-type:: annotation
				  hl-page:: 16
				  hl-color:: yellow
				  id:: 67c41806-1374-47e8-9422-ac782efdc2cc
			- Modern linguistics and AI, then, were “born” at about the same time, and grew up together, intersecting in a hybrid ﬁeld called **computational linguistics** or **natural language processing**.
			  hl-page:: 16
			  ls-type:: annotation
			  id:: 67c41854-53de-43c0-a05b-4df35c81126e
			  hl-color:: green
			  collapsed:: true
				- The problem of understanding language turned out to be considerably more complex than it seemed in 1957.
				  ls-type:: annotation
				  hl-page:: 17
				  hl-color:: yellow
				  id:: 67c41877-8b67-4d56-bc2f-f78ad7aacf03
				- Understanding language requires an understanding of the subject matter and context, not just an understanding of the structure of sentences. This might seem obvious, but it was not widely appreciated until the 1960s.
				  hl-page:: 17
				  ls-type:: annotation
				  id:: 67c418a4-dd39-415f-b94f-5deeb7503015
				  hl-color:: yellow
			- Much of the early work in **knowledge representation** (the study of how to put knowledge into a form that a computer can reason with) was tied to language and informed by research in linguistics, which was connected in turn to decades of work on the philosophical analysis of language.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: green
			  id:: 67c418cd-622f-4a5a-9952-017ea9b21d28
- 1.3 The History of Artiﬁcial Intelligence
  ls-type:: annotation
  hl-page:: 17
  hl-color:: red
  id:: 67c418ec-01ad-42e1-be11-4410be01704d
	- One quick way to summarize the milestones in AI history is to list the Turing Award winners:
	  ls-type:: annotation
	  hl-page:: 17
	  hl-color:: blue
	  id:: 67c419a6-b008-40db-b006-2069db8b5418
	  collapsed:: true
		- [[Marvin Minsky]] (1969) and [[John McCarthy]] (1971) for deﬁning the foundations of the ﬁeld based on representation and reasoning;
		  hl-page:: 17
		  ls-type:: annotation
		  id:: 67c419be-3d52-439a-941a-a6ce82fb13fd
		  hl-color:: blue
		- [[Allen Newell]] and [[Herbert Simon]] (1975) for symbolic models of problem solving and human cognition;
		  hl-page:: 17
		  ls-type:: annotation
		  id:: 67c419dd-d796-44a5-bfda-47b8b7c84a2b
		  hl-color:: blue
		- [[Ed Feigenbaum]] and [[Raj Reddy]] (1994) for developing expert systems that encode human knowledge to solve real-world problems;
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41a00-f2f2-4ab2-ae89-14a3779a3473
		- [[Judea Pearl]] (2011) for developing probabilistic reasoning techniques that deal with uncertainty in a principled manner; and
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41a1a-fc9a-4dcc-9c97-c42d8a1ab3c5
		- ﬁnally [[Yoshua Bengio]], [[Geoffrey Hinton]], and [[Yann LeCun]] (2019) for making “deep learning” (multilayer neural networks) a critical part of modern computing.
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41a2e-b9d0-4cf0-8d81-8d344790c36a
	- The rest of this section goes into more detail on each phase of AI history.
	  ls-type:: annotation
	  hl-page:: 17
	  hl-color:: blue
	  id:: 67c41a5c-a2d2-4f3f-aba2-6c36d82ff038
	- 1.3.1 The inception of artiﬁcial intelligence (1943–1956)
	  ls-type:: annotation
	  hl-page:: 17
	  hl-color:: red
	  id:: 67c41a6a-4d96-494a-ae92-16ffcb82a8ef
	  collapsed:: true
		- The ﬁrst work that is now generally recognized as AI was done by [[Warren McCulloch]] and [[Walter Pitts]] (1943).
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41a86-9d91-40c8-a142-2d39e7ffd4f6
		- Inspired by the mathematical modeling work of Pitts’s advisor [[Nicolas Rashevsky]] (1936, 1938), they drew on three sources
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41aaf-59da-498f-b9c6-fd65b9d2911c
		  collapsed:: true
			- knowledge of the basic physiology and function of neurons in the brain; a formal analysis of propositional logic due to [[Bertrand Russell]] and [[Whitehead]]; and [[Alan Turing]]’s theory of computation.
			  hl-page:: 17
			  ls-type:: annotation
			  id:: 67c41ac6-3fe7-46e0-a485-81ddf4a13781
			  hl-color:: yellow
			- They proposed a model of artiﬁcial neurons in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring in response to stimulation by a sufﬁcient number of neighboring neurons.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41af0-3191-4a9e-812f-2071632c9ed2
			- The state of a neuron was conceived of as “factually equivalent to a proposition which proposed its adequate stimulus.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41b17-5f63-4d85-89bf-65606177dfd7
			- They showed, for example, that any computable function could be computed by some network of connected neurons, and that all the logical connectives (AND, OR, NOT, etc.) could be implemented by simple network structures.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41b28-82f1-4a3d-9781-1263403f1f4b
			- McCulloch and Pitts also suggested that suitably deﬁned networks could learn.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41b41-87ce-4da1-8acb-df9a605c55f1
			- Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengths between neurons.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41b53-fa16-4ffd-99fa-421ddf1997b6
			- His rule, now called **Hebbian learning**, remains an inﬂuential model to this day.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: green
			  id:: 67c41b5e-aa59-4029-b6a2-dbbb34c66e1d
		- Two undergraduate students at Harvard, [[Marvin Minsky]] (1927–2016) and [[Dean Edmonds]], built the ﬁrst neural network computer in 1950.
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41b9f-37fd-4d2a-9aa6-b61ff8c1a0c9
		  collapsed:: true
			- The SNARC, as it was called, used3000 vacuum tubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of 40 neurons.
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41bc3-7167-48b1-b9c7-0634cb5742ff
			- Later, at Princeton, Minsky studied universal computation in neural networks. 
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41bdf-dc4d-4e40-b474-530c6df2c466
			- His Ph.D. committee was skeptical about whether this kind of work should be considered mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.”
			  ls-type:: annotation
			  hl-page:: 17
			  hl-color:: yellow
			  id:: 67c41bf8-cbb3-4a4b-8a59-ffd4d3d64de1
		- There were a number of other examples of early work that can be characterized as AI, including two checkers-playing programs developed independently in 1952 by [[Christopher Strachey]] at the University of Manchester and by [[Arthur Samuel]] at IBM.
		  ls-type:: annotation
		  hl-page:: 17
		  hl-color:: blue
		  id:: 67c41c1c-d598-4ac4-bbae-f0cb02de05a2
		  collapsed:: true
			- However, [[Alan Turing]]’s vision was the most inﬂuential. He gave lectures on the topic as early as 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950 article “Computing Machinery and Intelligence.” Therein, he introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning. He dealt with many of the objections raised to the possibility of AI, as described in Chapter 28. He also suggested that it would be easier to create human-level AI by developing learning algorithms and then teaching the machine rather than by programming its intelligence by hand. In subsequent lectures he warned that achieving this goal might not be the best thing for the human race.
			  hl-page:: 18
			  ls-type:: annotation
			  id:: 67c41c7f-55d0-4996-af91-b49a143a002a
			  hl-color:: yellow
		- In 1955, [[John McCarthy]] of Dartmouth College convinced [[Marvin Minsky]], [[Claude Shannon]], and [[Nathaniel Rochester]] to help him bring together U.S. researchers interested in automata theory, neural nets, and the study of intelligence.
		  hl-page:: 18
		  ls-type:: annotation
		  id:: 67c41cbb-390a-4e15-a2d4-1706ef1346f0
		  hl-color:: blue
		- They organized a two-month workshop at Dartmouth in the summer of 1956. There were 10 attendees in all, including [[Allen Newell]] and [[Herbert Simon]] from Carnegie Tech,[^11] [Trenchard More] from Princeton, [Arthur Samuel] from IBM, and [Ray Solomonoff] and [Oliver Selfridge] from MIT. The proposal states:[^12]
		  hl-page:: 18
		  ls-type:: annotation
		  id:: 67c41d08-a43b-440a-a475-910eda732f5f
		  hl-color:: blue
		  collapsed:: true
			- ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c41d45-ce99-4c39-b213-8aaecdfbe0d2
			  >We propose that a 2 month, 10 man study of artiﬁcial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to ﬁnd how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a signiﬁcant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.
			- hl-page:: 18
			  ls-type:: annotation
			  id:: 67c41d90-3c55-4bed-8854-ad7adacdd336
			  hl-color:: purple
			  [^11]:Now Carnegie Mellon University (CMU).
			- hl-page:: 18
			  ls-type:: annotation
			  id:: 67c41daa-fe2f-4847-8629-5cc14fba16a9
			  hl-color:: purple
			  [^12]:This was the ﬁrst ofﬁcial usage of McCarthy’s term artiﬁcial intelligence. Perhaps “computational rationality” would have been more precise and less threatening, but “AI” has stuck. At the 50th anniversary of the Dartmouth conference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert Wiener, who was promoting analog cybernetic devices rather than digital computers.
		- Despite this optimistic prediction, the Dartmouth workshop did not lead to any breakthroughs.
		  ls-type:: annotation
		  hl-page:: 18
		  hl-color:: blue
		  id:: 67c41dd0-cb16-4c08-9879-2d3471c1bf15
		  collapsed:: true
			- [[Allen Newell]] and [[Herbert Simon]] presented perhaps the most mature work, a mathematical theorem-proving system called the Logic Theorist (LT).
			  hl-page:: 18
			  ls-type:: annotation
			  id:: 67c41dfe-2f0c-49f1-abee-4527d717c220
			  hl-color:: yellow
			- Simon claimed, “We have invented a computer program capable of thinking non-numerically, and thereby solved the venerable mind–body problem.”[^13]
			  hl-page:: 18
			  ls-type:: annotation
			  id:: 67c41e2a-2fed-4636-81d1-b8418405eff9
			  hl-color:: yellow
				- hl-page:: 18
				  ls-type:: annotation
				  id:: 67c42134-a268-4916-abc5-9773f534811d
				  hl-color:: purple
				  [^13]:Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and translated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to each other as they wrote each instruction to make sure they agreed.
			- Soon after the workshop, the program was able to prove most of the theorems in Chapter 2 of Russell and Whitehead’s *Principia Mathematica*.
			  ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c41e48-4804-4363-bcb9-c785978412b5
			- Russell was reportedly delighted when told that LT had come up with a proof for one theorem that was shorter than the one in *Principia*.
			  ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c41e5d-31d3-425d-b907-500461108add
			- The editors of the *Journal of Symbolic Logic* were less impressed; they rejected a paper coauthored by Newell, Simon, and Logic Theorist.
			  ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c41e6c-2cec-4a2b-a74d-087e0419e6e0
	- 1.3.2 Early enthusiasm, great expectations (1952–1969)
	  ls-type:: annotation
	  hl-page:: 18
	  hl-color:: red
	  id:: 67c41e8f-3ae7-4ea4-bfa1-ef27ae2ede12
	  collapsed:: true
		- The intellectual establishment of the 1950s, by and large, preferred to believe that “a machine can never do X.” (See Chapter 28 for a long list of X’s gathered by Turing.) 
		  ls-type:: annotation
		  hl-page:: 18
		  hl-color:: blue
		  id:: 67c42159-4416-42e9-a487-264c98cac20b
		  collapsed:: true
			- AI researchers naturally responded by demonstrating one X after another. They focused in particular on tasks considered indicative of intelligence in humans, including games, puzzles, mathematics, and IQ tests.
			  ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c42171-5a11-4c2f-a4ac-329bc0dd7871
			- [[John McCarthy]] referred to this period as the “Look, Ma, no hands!” era.
			  ls-type:: annotation
			  hl-page:: 18
			  hl-color:: yellow
			  id:: 67c42179-ccab-4a69-94b4-7c053db97e05
			  hl-stamp:: 1740906878709
		- [[Allen Newell]] and [[Herbert Simon]] followed up their success with LT with the General Problem Solver, or GPS.
		  hl-page:: 19
		  ls-type:: annotation
		  id:: 67c42196-144a-4dc0-94bb-7adcd71882cc
		  hl-color:: blue
		  collapsed:: true
			- Unlike LT, this program was designed from the start to imitate human problem-solving protocols.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c421db-30c2-4240-ba09-4b7963df881d
			- Within the limited class of puzzles it could handle, it turned out that the order in which the program considered subgoals and possible actions was similar to that in which humans approached the same problems.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c421f9-bf0f-4321-ae4d-68cfd87065cd
		- Thus, GPS was probably the ﬁrst program to embody the “thinking humanly” approach. The success of GPS and subsequent programs as models of cognition led [[Allen Newell]] and [[Herbert Simon]] (1976) to formulate the famous **physical symbol system** hypothesis, which states that “a physical symbol system has the necessary and sufﬁcient means for general intelligent action.”
		  hl-page:: 19
		  ls-type:: annotation
		  id:: 67c4225a-9587-43c9-b866-b1bcb0e1e6a1
		  hl-color:: green
		  collapsed:: true
			- What they meant is that any system (human or machine) exhibiting intelligence must operate by manipulating data structures composed of symbols. We will see later that this hypothesis has been challenged from many directions.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c422e4-577e-4920-a0c1-8e64af522367
		- At IBM, [[Nathaniel Rochester]] and his colleagues produced some of the ﬁrst AI programs.
		  ls-type:: annotation
		  hl-page:: 19
		  hl-color:: blue
		  id:: 67c42319-4ff5-46d3-aa3b-311193f6384b
		- [[Herbert Gelernter]] (1959) constructed the Geometry Theorem Prover, which was able to prove theorems that many students of mathematics would ﬁnd quite tricky. This work was a precursor of modern mathematical theorem provers.
		  ls-type:: annotation
		  hl-page:: 19
		  hl-color:: blue
		  id:: 67c42330-6b55-4b90-ab18-fc0d92925c30
		- Of all the exploratory work done during this period, perhaps the most inﬂuential in the long run was that of [[Arthur Samuel]] on checkers (draughts).
		  ls-type:: annotation
		  hl-page:: 19
		  hl-color:: blue
		  id:: 67c4235c-ffd1-4623-9504-1776e2ae3300
		  collapsed:: true
			- Using methods that we now call reinforcement learning (see Chapter 23), Samuel’s programs learned to play at a strong amateur level.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c4238a-9d44-4cf1-93c9-a95ce11b938b
			- He thereby disproved the idea that computers can do only what they are told to: his program quickly learned to play a better game than its creator.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c423a1-afa3-4aa6-aea6-fe4def7769a2
			- The program was demonstrated on television in 1956, creating a strong impression.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c423ad-e069-4fa3-a5c0-5f45a5cd124c
			- Like Turing, Samuel had troubleﬁnding computer time. Working at night, he used machines that were still on the testing ﬂoor at IBM’s manufacturing plant.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c423e9-8844-4499-bed6-9d851b9fe52c
			- Samuel’s program was the precursor of later systems such as TD-GAMMON (Tesauro, 1992), which was among the world’s best backgammon players, and ALPHAGO (Silver et al., 2016), which shocked the world by defeating the human world champion at Go (see Chapter 6).
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c423f8-fb51-4312-97c3-24277f324919
		- In 1958, [[John McCarthy]] made two important contributions to AI. In MIT AI Lab Memo No. 1, he deﬁned the high-level language **Lisp**, which was to become the dominant AI programming language for the next 30 years.
		  hl-stamp:: 1740907612247
		  hl-page:: 19
		  ls-type:: annotation
		  id:: 67c4244c-4cb5-41f3-b0cc-281054978514
		  hl-color:: green
		  collapsed:: true
			- In a paper entitled *Programs with Common Sense*, he advanced a conceptual proposal for AI systems based on knowledge and reasoning.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c4247c-fdc2-441a-9aee-2bea2d3d00dc
			- The paper describes the Advice Taker, a hypothetical program that would embody general knowledge of the world and could use it to derive plans of action.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c4249e-f5e4-4665-a183-954e252890c5
			- The concept was illustrated with simple logical axioms that sufﬁce to generate a plan to drive to the airport. The program was also designed to accept new axioms in the normal course of operation, thereby allowing it to achieve competence in new areas *without being reprogrammed*.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c424c6-927c-4e75-b3e6-d567e92693e4
			- he Advice Taker thus embodied the central principles of knowledge representation and reasoning: that it is useful to have a formal, explicit representation of the world and its workings and to be able to manipulate that representation with deductive processes. The paper inﬂuenced the course of AI and remains relevant today.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c424e9-6f66-4e80-ad85-d415b712979e
		- 1958 also marked the year that [[Marvin Minsky]] moved to MIT. His initial collaboration with [[John McCarthy]] did not last, however.
		  ls-type:: annotation
		  hl-page:: 19
		  hl-color:: blue
		  id:: 67c42517-ef84-4c84-8e93-4ee9cea62dba
		  collapsed:: true
			- McCarthy stressed representation and reasoning in formal logic, whereas Minsky was more interested in getting programs to work and eventually developed an anti-logic outlook.
			  ls-type:: annotation
			  hl-page:: 19
			  hl-color:: yellow
			  id:: 67c4253e-b8cf-4a7c-a472-eaacdd0bfe02
			- In 1963, [[McCarthy]] started the AI lab at Stanford. His plan to use logic to build the ultimate Advice Taker was advanced by [[J. A. Robinson]]’s discovery in 1965 of the resolution method (a complete theorem-proving algorithm for ﬁrst-order logic; see Chapter 9).
			  hl-page:: 19
			  ls-type:: annotation
			  id:: 67c42764-0980-47ad-b116-6b2b1c0a2fe9
			  hl-color:: yellow
			- Work at Stanford emphasized general-purpose methods for logical reasoning. Applications of logic included [[Cordell Green]]’s question-answering and planning systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute (SRI).
			  ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c427da-a62f-4568-a9a6-26a8621ac464
			- The latter project, discussed further in Chapter 26, was the ﬁrst to demonstrate the complete integration of logical reasoning and physical activity.
			  ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c427f4-189a-493b-85a5-64f02891e613
		- At MIT, [[Marvin Minsky]] supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as **microworlds**.
		  hl-page:: 20
		  ls-type:: annotation
		  id:: 67c4283a-dbcd-4f4e-b863-1ff5c5d4a26c
		  hl-color:: green
		- [[James Slagle]]’s SAINT program (1963) was able to solve closed-form calculus integration problems typical of ﬁrst-year college courses.
		  ls-type:: annotation
		  hl-page:: 20
		  hl-color:: blue
		  id:: 67c42857-dd0b-4a17-85aa-332fe5bb1aed
		- [[Tom Evans]]’s ANALOGY program (1968) solved geometric analogy problems that appear in IQ tests.
		  ls-type:: annotation
		  hl-page:: 20
		  hl-color:: blue
		  id:: 67c42871-1afe-43a9-98b0-67e086c1ecfc
		- [[Daniel Bobrow]]’s STUDENT program (1967) solved algebra story problems, such as the following:
		  ls-type:: annotation
		  hl-page:: 20
		  hl-color:: blue
		  id:: 67c4288a-6ee9-4a0f-86a1-4bc80966f29a
		  collapsed:: true
			- ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c428a0-f760-4c50-af2b-f5cf3b032d04
			  >If the number of customers Tom gets is twice the square of 20 percent of the number of advertisements he runs, and the number of advertisements he runs is 45, what is the number of customers Tom gets?
		- The most famous microworld is the **blocks world**, which consists of a set of solid blocksBlocks world placed on a tabletop (or more often, a simulation of a tabletop), as shown in **Figure 1.3**.
		  hl-page:: 20
		  ls-type:: annotation
		  id:: 67c428ee-5a1c-44b1-bac0-7d0b89d99e0e
		  hl-color:: green
		  collapsed:: true
			- A typical task in this world is to rearrange the blocks in a certain way, using a robot hand that can pick up one block at a time.
			  ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c42941-38b0-4091-ab9b-53aa851b45b7
			- The blocks world was home to the vision project of [[David Huffman]] (1971), the vision and constraint-propagation work of [[David Waltz]] (1975), the learning theory of [[Patrick Winston]] (1970), the natural-language-understanding program of [[Terry Winograd]] (1972), and the planner of [[Scott Fahlman]] (1974).
			  ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c429d1-19e4-4b9f-b96d-7b934e36094c
			- [:span]
			  ls-type:: annotation
			  hl-page:: 20
			  hl-color:: yellow
			  id:: 67c42904-e122-47ab-b526-5e3639a06647
			  hl-type:: area
			  hl-stamp:: 1740908803132
		- Early work building on the neural networks of [[Warren McCulloch]] and [[Walter Pitts]] also ﬂourished. The work of [[Shmuel Winograd]] and [[Jack Cowan]] (1963) showed how a large number of elements could collectively represent an individual concept, with a corresponding increase in robustness and parallelism
		  hl-page:: 20
		  ls-type:: annotation
		  id:: 67c42a00-3718-420e-ab6f-13716bf6ae17
		  hl-color:: blue
		- [[Hebb]]s learning methods were enhanced by [[Bernie Widrow]] (Widrow and Hoff, 1960; Widrow, 1962), who called his networks **adalines**, and by [[Frank Rosenblatt]] (1962) with his **perceptrons**.
		  hl-page:: 21
		  ls-type:: annotation
		  id:: 67c42a3f-8364-43fd-9f02-86f7d02bcbe6
		  hl-color:: green
		- The [[perceptron convergence theorem]] (Block et al.,1962) says that the learning algorithm can adjust the connection strengths of a perceptron to match any input data, provided such a match exists.
		  ls-type:: annotation
		  hl-page:: 21
		  hl-color:: green
		  id:: 67c42a92-1ad5-43b1-9fb5-c62e25962aef
	- 1.3.3 A dose of reality (1966–1973)
	  ls-type:: annotation
	  hl-page:: 21
	  hl-color:: red
	  id:: 67c42aa7-4df6-45be-bc5e-6bb344f331b4
	  collapsed:: true
		- From the beginning, AI researchers were not shy about making predictions of their coming successes. The following statement by [[Herbert Simon]] in 1957 is often quoted:
		  ls-type:: annotation
		  hl-page:: 21
		  hl-color:: blue
		  id:: 67c42ac0-879c-4cdb-bd36-5e78fc660898
		  collapsed:: true
			- hl-page:: 21
			  ls-type:: annotation
			  id:: 67c42adc-45a5-4fa6-b4fb-d2a70a5f7095
			  hl-color:: yellow
			  >It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create. Moreover, their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.
		- The term “visible future” is vague, but [[Herbert Simon]] also made more concrete predictions: that within 10 years a computer would be chess champion and a signiﬁcant mathematical theorem would be proved by machine.
		  ls-type:: annotation
		  hl-page:: 21
		  hl-color:: blue
		  id:: 67c42ba3-fc7b-4b41-bb21-95f275a4abd6
		  collapsed:: true
			- These predictions came true (or approximately true) within 40 years rather than 10. Simon’s overconﬁdence was due to the promising performance of early AI systems on simple examples. In almost all cases, however, these early systems failed on more difﬁcult problems.
			  ls-type:: annotation
			  hl-page:: 21
			  hl-color:: yellow
			  id:: 67c42bc2-e2b4-4a4e-8a9b-96af45d10495
		- There were two main reasons for this failure:
		  hl-page:: 21
		  ls-type:: annotation
		  id:: 67c42be2-a053-4e6d-acbd-706497ab0d03
		  hl-color:: blue
		  collapsed:: true
			- The ﬁrst was that many early AI systems were based primarily on “informed introspection” as to how humans perform a task, rather than on a careful analysis of the task, what it means to be a solution, and what an algorithm would need to do to reliably produce such solutions.
			  ls-type:: annotation
			  hl-page:: 21
			  hl-color:: blue
			  id:: 67c42bf2-36c5-4f2f-8ccd-af1a07a512a6
			- The second reason for failure was a lack of appreciation of the intractability of many of the problems that AI was attempting to solve.
			  ls-type:: annotation
			  hl-page:: 21
			  hl-color:: blue
			  id:: 67c42c03-16a3-46eb-8231-82d0a72f8854
			  collapsed:: true
				- Most of the early problem-solving systems worked by trying out different combinations of steps until the solution was found. This strategy worked initially because microworlds contained very few objects and hence very few possible actions and very short solution sequences.
				  ls-type:: annotation
				  hl-page:: 21
				  hl-color:: yellow
				  id:: 67c42c11-c490-4cd3-9704-a231f76fc278
				- Before the theory of computational complexity was developed, it was widely thought that “scaling up” to larger problems was simply a matter of faster hardware and larger memories.
				  ls-type:: annotation
				  hl-page:: 21
				  hl-color:: yellow
				  id:: 67c42c29-3901-4acd-b052-4a0c8435a9d4
				- The optimism that accompanied the development of resolution theorem proving, for example, was soon dampened when researchers failed to prove theorems involving more than a few dozen facts.
				  ls-type:: annotation
				  hl-page:: 21
				  hl-color:: yellow
				  id:: 67c42c39-2283-464e-a64d-0c67c5a69ba1
				- _The fact that a program can Jﬁnd a solution in principle does not mean that the program contains any of the mechanisms needed to ﬁnd it in practice._
				  ls-type:: annotation
				  hl-page:: 21
				  hl-color:: yellow
				  id:: 67c42c43-eefd-4d88-b060-f80f7682824c
		- The illusion of unlimited computational power was not conﬁned to problem-solving programs.
		  ls-type:: annotation
		  hl-page:: 21
		  hl-color:: blue
		  id:: 67c42cc6-8145-4ceb-ac62-57be4c925e17
		- Early experiments in **machine evolution** (now called **genetic programming**) (Friedberg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by making an appropriate series of small mutations to a machine-code program, one can generate a program with good performance for any particular task.
		  hl-page:: 21
		  ls-type:: annotation
		  id:: 67c42cdd-90f2-4146-9328-7aa7c1f4754f
		  hl-color:: blue
		  collapsed:: true
			- The idea, then, was to try random mutations with a selection process to preserve mutations that seemed useful. Despite thousands of hours of CPU time, almost no progress was demonstrated.
			  ls-type:: annotation
			  hl-page:: 21
			  hl-color:: yellow
			  id:: 67c42d15-4077-4f38-824d-cc38635d77d3
		- Failure to come to grips with the “combinatorial explosion” was one of the main criticisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the decision by the British government to end support for AI research in all but two universities.
		  hl-page:: 21
		  ls-type:: annotation
		  id:: 67c42d29-f3a2-4f44-b001-ddb7ef40f723
		  hl-color:: blue
		  collapsed:: true
			- (Oral tradition paints a somewhat different and more colorful picture, with political ambitions and personal animosities whose description is beside the point.
			  ls-type:: annotation
			  hl-page:: 22
			  hl-color:: yellow
			  id:: 67c42d5f-7993-460f-9806-f77ed2f21586
		- A third difﬁculty arose because of some fundamental limitations on the basic structures being used to generate intelligent behavior.
		  ls-type:: annotation
		  hl-page:: 22
		  hl-color:: blue
		  id:: 67c42dd2-953f-44d8-a044-9e62953bcdbe
		  collapsed:: true
			- For example, Minsky and Papert’s book *Perceptrons* (1969) proved that, although perceptrons (a simple form of neural network) could be shown to learn anything they were capable of representing, they could represent very little.
			  ls-type:: annotation
			  hl-page:: 22
			  hl-color:: yellow
			  id:: 67c42dea-2008-45db-8902-839b64682c75
			  hl-stamp:: 1740910063091
			- In particular, a two-input perceptron could not be trained to recognize when its two inputs were different. Although their results did not apply to more complex, multilayer networks, research funding for neural-net research soon dwindled to almost nothing.
			  ls-type:: annotation
			  hl-page:: 22
			  hl-color:: yellow
			  id:: 67c42e51-2ab2-4fcc-a676-6d863b21910f
			- Ironically, the new back-propagation learning algorithms that were to cause an enormous resurgence in neural-net research in the late 1980s and again in the 2010s had already been developed in other contexts in the early 1960s (Kelley, 1960; Bryson, 1962).
			  ls-type:: annotation
			  hl-page:: 22
			  hl-color:: yellow
			  id:: 67c42e70-7335-46fa-9eec-132c54f9d923
- 1.3.4 Expert systems (1969–1986)
  ls-type:: annotation
  hl-page:: 22
  hl-color:: red
  id:: 67c42e93-29e6-425b-804a-7d3ae2483cf7
	- The picture of problem solving that had arisen during the ﬁrst decade of AI research was of a general-purpose search mechanism trying to string together elementary reasoning steps toﬁnd complete solutions. 
	  ls-type:: annotation
	  hl-page:: 22
	  hl-color:: blue
	  id:: 67c42f0a-a789-4376-9419-e0c3695e90af
	- Such approaches have been called **weak methods** because, although general, they do not scale up to large or difﬁcult problem instances.
	  hl-page:: 22
	  ls-type:: annotation
	  id:: 67c42f25-6c7b-4eb5-9b05-704e66bcbdca
	  hl-color:: green
	  collapsed:: true
		- The alternative to weak methods is to use more powerful, domain-speciﬁc knowledge that allows larger reasoning steps and can more easily handle typically occurring cases in narrow areas of expertise. One might say that to solve a hard problem, you have to almost know the answer already.
		  ls-type:: annotation
		  hl-page:: 22
		  hl-color:: yellow
		  id:: 67c42f4b-ec24-4e0f-a44f-d52516cf7972
	- The DENDRAL program (Buchanan et al., 1969) was an early example of this approach. It was developed at Stanford, where [[Ed Feigenbaum]] (a former student of [[Herbert Simon]]), [[Bruce Buchanan]] (a philosopher turned computer scientist), and [[Joshua Lederberg]] (a Nobel laureate geneticist) teamed up to solve the problem of inferring molecular structure from the information provided by a mass spectrometer.
	  ls-type:: annotation
	  hl-page:: 22
	  hl-color:: blue
	  id:: 67c42f6d-eab5-4d72-94f2-e737bd7713ab
	  collapsed:: true
		- The input to the program consists of the elementary formula of the molecule (e.g., *C_{6}H_{13}NO_{2}*) and the mass spectrum giving the masses of the various fragments of the molecule generated when it is bombarded by an electron beam.
		  hl-page:: 22
		  ls-type:: annotation
		  id:: 67c42f9d-0448-4c6b-baca-444c7d5a9cc8
		  hl-color:: yellow
		- For example, the mass spectrum might contain a peak at *m* = 15, corresponding to the mass of a methyl (*CH_{3}*) fragment.
		  hl-page:: 22
		  ls-type:: annotation
		  id:: 67c42fcf-a38d-49ce-8ffc-079d0e8a2d58
		  hl-color:: yellow
- The naive version of the program generated all possible structures consistent with the formula, and then predicted what mass spectrum would be observed for each, comparing this with the actual spectrum.
  ls-type:: annotation
  hl-page:: 22
  hl-color:: blue
  id:: 67c43018-bec0-41c4-a26c-42326d1f1c94
- As one might expect, this is intractable for even moderate-sized molecules. The DENDRAL researchers consulted analytical chemists and found that they worked by looking for well-known patterns of peaks in the spectrum that suggested common substructures in the molecule. For example, the following rule is used to recognize a ketone(*C*=O) subgroup (which weighs 28):
  ls-type:: annotation
  hl-page:: 22
  hl-color:: yellow
  id:: 67c43102-7b26-451d-8881-d71eed86b863
	- hl-page:: 22
	  ls-type:: annotation
	  id:: 67c43120-775c-4895-be1a-7b2f132d7e5e
	  hl-color:: yellow
	  >**if** *M* is the mass of the whole molecule and there are two peaks at *x*_{1} and *x*{2} such that (a) *x*_{1} + *x*_{2} = *M* + 28; (b) *x*_{1} − 28 is a high peak; (c) *x*_{2} − 28 is a high peak; and (d) At least one of *x*_{1} and *x*_{2} is high then there is a ketone subgroup.
- Recognizing that the molecule contains a particular substructure reduces the number of possible candidates enormously.
  ls-type:: annotation
  hl-page:: 22
  hl-color:: blue
  id:: 67c431b0-f314-45c4-8803-034bddd830fe